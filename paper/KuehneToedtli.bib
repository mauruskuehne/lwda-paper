
@inproceedings{yu_generating_2018,
	title = {Generating {Adversarial} {Examples} {With} {Conditional} {Generative} {Adversarial} {Net}},
	doi = {10.1109/ICPR.2018.8545152},
	abstract = {Recently, deep neural networks have significant progress and successful application in various fields, but they are found vulnerable to attack instances, e.g., adversarial examples. State-of-art attack methods can generate attack images by adding small perturbation to the source image. These attack images can fool the classifier but have little impact to human. Therefore, such attack instances are difficult to generate by searching the feature space. How to design an effective and robust generating method has become a spotlight. Inspired by adversarial examples, we propose two novel generative models to produce adaptive attack instances directly, in which conditional generative adversarial network is adopted and distinctive strategy is designed for training. Compared with the common method, such as Fast Gradient Sign Method, our models can reduce the generating cost and improve robustness and has about one fifth running time for producing attack instance.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Yu, P. and Song, K. and Lu, J.},
	month = aug,
	year = {2018},
	keywords = {neural nets, deep neural networks, Training, Computational modeling, Data models, Machine learning, adaptive attack instances, adversarial examples, attack images, conditional generative adversarial net, conditional generative adversarial network, DNN attack, generative adversarial network, Generative adversarial networks, generative models, Generators, image classification, Perturbation methods},
	pages = {676--681},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/884KMGCG/8545152.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/YDFEXAJX/Yu et al. - 2018 - Generating Adversarial Examples With Conditional G.pdf:application/pdf}
}

@inproceedings{bose_adversarial_2018,
	title = {Adversarial {Attacks} on {Face} {Detectors} {Using} {Neural} {Net} {Based} {Constrained} {Optimization}},
	doi = {10.1109/MMSP.2018.8547128},
	abstract = {Adversarial attacks involve adding, small, often imperceptible, perturbations to inputs with the goal of getting a machine learning model to misclassifying them. While many different adversarial attack strategies have been proposed on image classification models, object detection pipelines have been much harder to break. In this paper, we propose a novel strategy to craft adversarial examples by solving a constrained optimization problem using an adversarial generator network. Our approach is fast and scalable, requiring only a forward pass through our trained generator network to craft an adversarial sample. Unlike in many attack strategies we show that the same trained generator is capable of attacking new images without explicitly optimizing on them. We evaluate our attack on a trained Faster R-CNN face detector on the cropped 300-W face dataset where we manage to reduce the number of detected faces to 0.5\% of all originally detected faces. In a different experiment, also on 300-W, we demonstrate the robustness of our attack to a JPEG compression based defense typical JPEG compression level of 75\% reduces the effectiveness of our attack from only 0.5\% of detected faces to a modest 5.0\%.},
	booktitle = {2018 {IEEE} 20th {International} {Workshop} on {Multimedia} {Signal} {Processing} ({MMSP})},
	author = {Bose, A. J. and Aarabi, P.},
	month = aug,
	year = {2018},
	keywords = {learning (artificial intelligence), convolution, feedforward neural nets, optimisation, Computational modeling, Deep Learning, Generators, image classification, Perturbation methods, adversarial attack strategies, Adversarial Attacks, adversarial generator network, data compression, detection pipelines, Detectors, Face, face dataset, Face Detection, image classification models, image coding, imperceptible perturbations, JPEG compression, machine learning model, neural net based constrained optimization problem, object detection, Object Detection, object detection pipelines, Optimization, Proposals, R-CNN face detector},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/BUSHGKR2/8547128.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/K9NLCT56/Bose and Aarabi - 2018 - Adversarial Attacks on Face Detectors Using Neural.pdf:application/pdf}
}

@inproceedings{zhao_reinforced_2018,
	title = {Reinforced {Adversarial} {Attacks} on {Deep} {Neural} {Networks} {Using} {ADMM}},
	doi = {10.1109/GlobalSIP.2018.8646651},
	abstract = {As deep learning penetrates into wide application domains, it is essential to evaluate the robustness of deep neural networks (DNNs) under adversarial attacks, especially for some security-critical applications. To better understand the security properties of DNNs, we propose a general framework for constructing adversarial examples, based on ADMM (Alternating Direction Method of Multipliers). This general framework can be adapted to implement L2 and L0 attacks with minor changes. Our ADMM attacks require less distortion for incorrect classification compared with C\&W attacks. Our ADMM attack is also able to break defenses such as defensive distillation and adversarial training, and provide strong attack transferability.},
	booktitle = {2018 {IEEE} {Global} {Conference} on {Signal} and {Information} {Processing} ({GlobalSIP})},
	author = {Zhao, P. and Xu, K. and Zhang, T. and Fardad, M. and Wang, Y. and Lin, X.},
	month = {November},
	year = {2018},
	keywords = {learning (artificial intelligence), neural nets, deep neural networks, Neural networks, security of data, adversarial examples, Adversarial Attacks, Optimization, ADMM (Alternating Direction Method of Multipliers), ADMM attack, adversarial training, alternating direction method, attack transferability, C\&W attacks, Convex functions, deep learning penetrates, Deep Neural Networks, Distortion, Distortion measurement, DNNs, Law, reinforced adversarial attacks, security properties, security-critical applications},
	pages = {1169--1173},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/WKG4LL2F/8646651.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/7SDVGJQ3/Zhao et al. - 2018 - Reinforced Adversarial Attacks on Deep Neural Netw.pdf:application/pdf}
}

@article{yuan_adversarial_2019,
	title = {Adversarial {Examples}: {Attacks} and {Defenses} for {Deep} {Learning}},
	issn = {2162-237X},
	shorttitle = {Adversarial {Examples}},
	doi = {10.1109/TNNLS.2018.2886017},
	abstract = {With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks (DNNs) have been recently found vulnerable to well-designed input samples called adversarial examples. Adversarial perturbations are imperceptible to human but can easily fool DNNs in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying DNNs in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for DNNs, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples. In addition, three major challenges in adversarial examples and the potential solutions are discussed.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Yuan, X. and He, P. and Zhu, Q. and Li, X.},
	year = {2019},
	keywords = {Deep learning, Computational modeling, Data models, Feature extraction, Security, Adversarial examples, Computer architecture, deep learning (DL), deep neural network (DNN), security., Task analysis},
	pages = {1--20},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/6ILSSIBX/8611298.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/VUXHDHGU/Yuan et al. - 2019 - Adversarial Examples Attacks and Defenses for Dee.pdf:application/pdf}
}

@inproceedings{zawistowski_adversarial_2018,
	title = {Adversarial examples: {A} survey},
	shorttitle = {Adversarial examples},
	doi = {10.23919/URSI.2018.8406730},
	abstract = {Adversarial examples are a phenomenon that have gathered a lot of attention in recent studies. The fact that the addition of very small, but carefully crafted perturbations to the inputs of sophisticated and high performing machine learning models may cause them to make significant errors, is both fascinating and important. A survey of findings connected with adversarial examples is presented and discussed in this paper.},
	booktitle = {2018 {Baltic} {URSI} {Symposium} ({URSI})},
	author = {Zawistowski, P.},
	month = may,
	year = {2018},
	keywords = {learning (artificial intelligence), Training, Computational modeling, Machine learning, Robustness, adversarial examples, Perturbation methods, Task analysis, machine learning models, Noise measurement},
	pages = {295--298},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/57U94WGB/8406730.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/ZHNAT3X9/Zawistowski - 2018 - Adversarial examples A survey.pdf:application/pdf}
}

@article{akhtar_threat_2018,
	title = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}},
	doi = {10.1109/ACCESS.2018.2807385},
	abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	journal = {IEEE Access},
	author = {Akhtar, N. and Mian, A.},
	year = {2018},
	keywords = {learning (artificial intelligence), neural nets, deep neural networks, Deep learning, Neural networks, Computational modeling, Predictive models, Machine learning, Computer vision, Perturbation methods, Task analysis, artificial intelligence, adversarial attacks, adversarial learning, adversarial perturbation, black-box attack, computer vision, deep learning models, humanities, perturbation detection, self-driving cars, white-box attack},
	pages = {14410--14430},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/QAFKQXMR/8294186.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/V4WZ6GXE/Akhtar and Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf:application/pdf}
}

@misc{wu_beginners_2019,
	title = {A {Beginner}’s {Tutorial} on {Building} an {AI} {Image} {Classifier} using {PyTorch}},
	url = {https://towardsdatascience.com/a-beginners-tutorial-on-building-an-ai-image-classifier-using-pytorch-6f85cb69cba7},
	abstract = {This is a step-by-step guide to build an image classifier. The AI model will be able to learn to label images. I use Python and Pytorch.},
	language = {en},
	urldate = {2019-07-28},
	journal = {Medium},
	author = {Wu, Alexander},
	month = jun,
	year = {2019},
	file = {Snapshot:/Users/maumau/Zotero/storage/2F5C82FD/a-beginners-tutorial-on-building-an-ai-image-classifier-using-pytorch-6f85cb69cba7.html:text/html}
}

@misc{noauthor_lasagne_2019,
	title = {Lasagne recipes: examples, {IPython} notebooks, ... {Contribute} to {Lasagne}/{Recipes} development by creating an account on {GitHub}},
	shorttitle = {Lasagne recipes},
	url = {https://github.com/Lasagne/Recipes},
	urldate = {2019-07-28},
	publisher = {Lasagne},
	month = jul,
	year = {2019},
	note = {original-date: 2015-05-21T11:56:18Z}
}

@misc{noauthor_image_nodate,
	title = {Image {Classification} using {Pre}-trained {Models} in {PyTorch} {\textbar} {Learn} {OpenCV}},
	url = {https://www.learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/},
	abstract = {In this blog, we will cover how we can use TorchVision module to load pre-trained models and carry out model inference to classify an image.},
	language = {en-US},
	urldate = {2019-07-28},
	file = {Snapshot:/Users/maumau/Zotero/storage/CG6J6AMJ/pytorch-for-beginners-image-classification-using-pre-trained-models.html:text/html}
}

@article{baluja_adversarial_2017,
	title = {Adversarial {Transformation} {Networks}: {Learning} to {Generate} {Adversarial} {Examples}},
	shorttitle = {Adversarial {Transformation} {Networks}},
	url = {http://arxiv.org/abs/1703.09387},
	abstract = {Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier's outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.},
	urldate = {2019-08-06},
	journal = {arXiv:1703.09387 [cs]},
	author = {Baluja, Shumeet and Fischer, Ian},
	month = {März},
	year = {2017},
	note = {arXiv: 1703.09387},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1703.09387 PDF:/Users/maumau/Zotero/storage/KXMPSI69/Baluja and Fischer - 2017 - Adversarial Transformation Networks Learning to G.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/C6TN5EGH/1703.html:text/html}
}

@article{sarkar_upset_2017,
	title = {{UPSET} and {ANGRI} : {Breaking} {High} {Performance} {Image} {Classifiers}},
	shorttitle = {{UPSET} and {ANGRI}},
	url = {http://arxiv.org/abs/1707.01159},
	abstract = {In this paper, targeted fooling of high performance image classifiers is achieved by developing two novel attack methods. The first method generates universal perturbations for target classes and the second generates image specific perturbations. Extensive experiments are conducted on MNIST and CIFAR10 datasets to provide insights about the proposed algorithms and show their effectiveness.},
	urldate = {2019-08-06},
	journal = {arXiv:1707.01159 [cs]},
	author = {Sarkar, Sayantan and Bansal, Ankan and Mahbub, Upal and Chellappa, Rama},
	month = {Juli},
	year = {2017},
	note = {arXiv: 1707.01159},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1707.01159 PDF:/Users/maumau/Zotero/storage/3H7SMS9Z/Sarkar et al. - 2017 - UPSET and ANGRI  Breaking High Performance Image .pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/E4NHPPZY/1707.html:text/html}
}

@misc{kratzert_frederick_finetuning_2017,
	title = {Finetuning {AlexNet} with {TensorFlow}},
	url = {https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html},
	urldate = {2019-08-09},
	author = {{Kratzert, Frederick}},
	month = feb,
	year = {2017}
}

@inproceedings{chatfield_return_2014,
	title = {Return of the {Devil} in the {Details}: {Delving} {Deep} into {Convolutional} {Nets}},
	booktitle = {British {Machine} {Vision} {Conference}},
	author = {Chatfield, K. and Simonyan, K. and Vedaldi, A. and Zisserman, A.},
	year = {2014}
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	doi = {10.1007/s11263-015-0816-y},
	number = {3},
	journal = {International Journal of Computer Vision (IJCV)},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	year = {2015},
	pages = {211--252}
}

@article{hayes_learning_2017,
	title = {Learning {Universal} {Adversarial} {Perturbations} with {Generative} {Models}},
	url = {http://arxiv.org/abs/1708.05207},
	abstract = {Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it's generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks.},
	urldate = {2019-08-09},
	journal = {arXiv:1708.05207 [cs, stat]},
	author = {Hayes, Jamie and Danezis, George},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.05207},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1708.05207 PDF:/Users/maumau/Zotero/storage/KXCRZA47/Hayes and Danezis - 2017 - Learning Universal Adversarial Perturbations with .pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/8LZVZKAY/1708.html:text/html}
}

@inproceedings{shirataki_study_2017,
	title = {A study on interpretability of decision of machine learning},
	doi = {10.1109/BigData.2017.8258557},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Shirataki, S. and Yamaguchi, S.},
	month = dec,
	year = {2017},
	keywords = {learning (artificial intelligence), Training, Big Data, data analysis, machine learning, Predictive models, Support vector machines, support vector machines, Analytical models, Tools, big data analysis, black boxes, decision reasons, DVD, highly weighted words, interpretability, opinions classification, pattern classification, Support Vector Machine, SVM, text analysis},
	pages = {4830--4831}
}

@article{falcini_deep_2017,
	title = {Deep {Learning} in {Automotive} {Software}},
	volume = {34},
	issn = {0740-7459},
	doi = {10.1109/MS.2017.79},
	number = {3},
	journal = {IEEE Software},
	author = {Falcini, F. and Lami, G. and Costanza, A. M.},
	month = may,
	year = {2017},
	keywords = {artificial neural networks, Artificial neural networks, Biological neural networks, learning (artificial intelligence), neural nets, deep learning, deep neural networks, Machine learning, neural networks, artificial intelligence, computer vision, ad hoc training data, ANNs, Automotive electronics, automotive engineering, automotive software engineering, automotive SPICE, Automotive SPICE, computing methodologies, data-intensive deep neural network training, Deep leanring, development lifecycle, DNN training, intelligent transportation systems, ISO 26262, ISO/AWI PAS 21448, software development, Software development, software engineering, software engineering process, software process improvement, Software Process Improvement and Capability Determination, standards, V model, vision and scene understanding, W model},
	pages = {56--63}
}

@article{su_one_2017,
	title = {One pixel attack for fooling deep neural networks},
	url = {http://arxiv.org/abs/1710.08864},
	abstract = {Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 68.36\% of the natural images in Kaggle CIFAR-10 test dataset and 16.04\% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 73.22\% and 22.91\% confidence on average. We also show the same vulnerability on original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
	urldate = {2019-09-02},
	journal = {arXiv:1710.08864 [cs, stat]},
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	month = {Oktober},
	year = {2017},
	note = {arXiv: 1710.08864},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1710.08864 PDF:/Users/maumau/Zotero/storage/9UKTG4EY/Su et al. - 2017 - One pixel attack for fooling deep neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/C5AA4SPG/1710.html:text/html}
}

@book{martin_abadi_tensorflow:_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	year = {2015},
	annote = {Software available from tensorflow.org}
}

@article{jia_caffe:_2014,
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	journal = {arXiv preprint arXiv:1408.5093},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	year = {2014}
}

@article{dargan_survey_2019,
	title = {A {Survey} of {Deep} {Learning} and {Its} {Applications}: {A} {New} {Paradigm} to {Machine} {Learning}},
	issn = {1886-1784},
	url = {https://doi.org/10.1007/s11831-019-09344-w},
	doi = {10.1007/s11831-019-09344-w},
	abstract = {Nowadays, deep learning is a current and a stimulating field of machine learning. Deep learning is the most effective, supervised, time and cost efficient machine learning approach. Deep learning is not a restricted learning approach, but it abides various procedures and topographies which can be applied to an immense speculum of complicated problems. The technique learns the illustrative and differential features in a very stratified way. Deep learning methods have made a significant breakthrough with appreciable performance in a wide variety of applications with useful security tools. It is considered to be the best choice for discovering complex architecture in high-dimensional data by employing back propagation algorithm. As deep learning has made significant advancements and tremendous performance in numerous applications, the widely used domains of deep learning are business, science and government which further includes adaptive testing, biological image classification, computer vision, cancer detection, natural language processing, object detection, face recognition, handwriting recognition, speech recognition, stock market analysis, smart city and many more. This paper focuses on the concepts of deep learning, its basic and advanced architectures, techniques, motivational aspects, characteristics and the limitations. The paper also presents the major differences between the deep learning, classical machine learning and conventional learning approaches and the major challenges ahead. The main intention of this paper is to explore and present chronologically, a comprehensive survey of the major applications of deep learning covering variety of areas, study of the techniques and architectures used and further the contribution of that respective application in the real world. Finally, the paper ends with the conclusion and future aspects.},
	journal = {Archives of Computational Methods in Engineering},
	author = {Dargan, Shaveta and Kumar, Munish and Ayyagari, Maruthi Rohit and Kumar, Gulshan},
	month = {Juni},
	year = {2019}
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2019-09-03},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = {September},
	year = {2014},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1409.1556 PDF:/Users/maumau/Zotero/storage/UUU3PWDY/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/244EPB4G/1409.html:text/html}
}

@article{liu_survey_2018,
	title = {A {Survey} on {Security} {Threats} and {Defensive} {Techniques} of {Machine} {Learning}: {A} {Data} {Driven} {View}},
	volume = {6},
	shorttitle = {A {Survey} on {Security} {Threats} and {Defensive} {Techniques} of {Machine} {Learning}},
	doi = {10.1109/ACCESS.2018.2805680},
	abstract = {Machine learning is one of the most prevailing techniques in computer science, and it has been widely applied in image processing, natural language processing, pattern recognition, cybersecurity, and other fields. Regardless of successful applications of machine learning algorithms in many scenarios, e.g., facial recognition, malware detection, automatic driving, and intrusion detection, these algorithms and corresponding training data are vulnerable to a variety of security threats, inducing a significant performance decrease. Hence, it is vital to call for further attention regarding security threats and corresponding defensive techniques of machine learning, which motivates a comprehensive survey in this paper. Until now, researchers from academia and industry have found out many security threats against a variety of learning algorithms, including naive Bayes, logistic regression, decision tree, support vector machine (SVM), principle component analysis, clustering, and prevailing deep neural networks. Thus, we revisit existing security threats and give a systematic survey on them from two aspects, the training phase and the testing/inferring phase. After that, we categorize current defensive techniques of machine learning into four groups: security assessment mechanisms, countermeasures in the training phase, those in the testing or inferring phase, data security, and privacy. Finally, we provide five notable trends in the research on security threats and defensive techniques of machine learning, which are worth doing in-depth studies in future.},
	journal = {IEEE Access},
	author = {Liu, Q. and Li, P. and Zhao, W. and Cai, W. and Yu, S. and Leung, V. C. M.},
	year = {2018},
	keywords = {learning (artificial intelligence), neural nets, deep neural networks, Training, security of data, machine learning, Machine learning, Support vector machines, principal component analysis, Security, support vector machines, Testing, Machine learning algorithms, regression analysis, pattern classification, SVM, adversarial samples, automatic driving, Bayes methods, clustering, computer science, cybersecurity, data driven view learning, data security, decision tree, decision trees, defensive techniques, facial recognition, image processing, intrusion detection, invasive software, logistic regression, malware detection, naive Bayes, natural language processing, pattern recognition, principle component analysis, security threats, support vector machine, Taxonomy, Training data},
	pages = {12103--12117},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/K836TWS2/Liu et al. - 2018 - A Survey on Security Threats and Defensive Techniq.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/27IDF4KM/Liu et al. - 2018 - A Survey on Security Threats and Defensive Techniq.pdf:application/pdf}
}

@article{fawzi_robustness_2017,
	title = {The {Robustness} of {Deep} {Networks}: {A} {Geometrical} {Perspective}},
	volume = {34},
	shorttitle = {The {Robustness} of {Deep} {Networks}},
	doi = {10.1109/MSP.2017.2740965},
	abstract = {Deep neural networks have recently shown impressive classification performance on a diverse set of visual tasks. When deployed in real-world (noise-prone) environments, it is equally important that these classifiers satisfy robustness guarantees: small perturbations applied to the samples should not yield significant loss to the performance of the predictor. The goal of this article is to discuss the robustness of deep networks to a diverse set of perturbations that may affect the samples in practice, including adversarial perturbations, random noise, and geometric transformations. This article further discusses the recent works that build on the robustness analysis to provide geometric insights on the classifier's decision surface, which help in developing a better understanding of deep networks. Finally, we present recent solutions that attempt to increase the robustness of deep networks. We hope this review article will contribute to shedding light on the open research challenges in the robustness of deep networks and stir interest in the analysis of their fundamental properties.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Fawzi, A. and Moosavi-Dezfooli, S. and Frossard, P.},
	month = nov,
	year = {2017},
	keywords = {neural nets, deep neural networks, Neural networks, Machine learning, Robustness, image classification, adversarial perturbations, Classification, classifier decision surface, geometric transformations, geometrical perspective, impressive classification performance, noise-prone environments, predictor performance, random noise, real-world environments, robustness analysis, visual tasks, Visualization},
	pages = {50--62},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/AHMHN7A8/Fawzi et al. - 2017 - The Robustness of Deep Networks A Geometrical Per.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/V76MAWEA/Fawzi et al. - 2017 - The Robustness of Deep Networks A Geometrical Per.pdf:application/pdf}
}

@inproceedings{min_adversarial_2018,
	title = {Adversarial {Attack}? {Don}'t {Panic}},
	shorttitle = {Adversarial {Attack}?},
	doi = {10.1109/BIGCOM.2018.00021},
	abstract = {Deep learning is playing a more and more important role in our daily life and scientific research such as autonomous systems, intelligent life and data mining. However, numerous studies have showed that deep learning with superior performance on many tasks may suffer from subtle perturbations constructed by attacker purposely, called adversarial perturbations, which are imperceptible to human observers but completely effect deep neural network models. The emergence of adversarial attacks has led to questions about neural networks. Therefore, machine learning security and privacy are becoming an increasingly active research area. In this paper, we summarize the prevalent methods for the generating adversarial attacks according to three groups. We elaborated on their ideas and principles of generation. We further analyze the common limitations of these methods and implement statistical experiments of the last layer output on CleverHans to reveal that the detection of adversarial samples is not as difficult as it seems and can be achieved in some relatively simple manners.},
	booktitle = {2018 4th {International} {Conference} on {Big} {Data} {Computing} and {Communications} ({BIGCOM})},
	author = {Min, F. and Qiu, X. and Wu, F.},
	month = aug,
	year = {2018},
	keywords = {learning (artificial intelligence), neural nets, deep learning, Neural networks, security of data, data mining, Machine learning, Machine learning algorithms, Perturbation methods, Manifolds, adversarial perturbations, adversarial attack, autonomous systems, Classification algorithms, data privacy, deep learning, adversarial attacks, adversarial generation algorithms, easy detection, deep neural network models, intelligent life, Iterative methods, machine learning privacy, machine learning security},
	pages = {90--95},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/B8PMS78V/Min et al. - 2018 - Adversarial Attack Don't Panic.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/KMAATSMP/Min et al. - 2018 - Adversarial Attack Don't Panic.pdf:application/pdf}
}

@inproceedings{akhtar_defense_2018,
	title = {Defense {Against} {Universal} {Adversarial} {Perturbations}},
	doi = {10.1109/CVPR.2018.00357},
	abstract = {Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to 'any' image can fool a state-of-the-art network classifier to change its prediction about the image label. These 'Universal Adversarial Perturbations' pose a serious threat to the success of Deep Learning in practice. We present the first dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as 'pre-input' layers to a targeted model, such that the targeted model needs no modification. The PRN is learned from real and synthetic image-agnostic perturbations, where an efficient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is first passed through the PRN and verified by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the network classifiers against unseen adversarial perturbations in the real-world scenarios with up to 97.5\% success rate. The PRN also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Akhtar, N. and Liu, J. and Mian, A.},
	month = jun,
	year = {2018},
	keywords = {learning (artificial intelligence), deep learning, Training, Neural networks, Computational modeling, Robustness, Perturbation methods, Detectors, Integrated circuits, pattern classification, image processing, discrete cosine transform, discrete cosine transforms, image label, label prediction, network classifiers, perturbation detector, perturbation rectifying network, PRN, query image, synthetic image-agnostic perturbations, targeted network, universal adversarial perturbations},
	pages = {3389--3398},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/636U8N3F/Akhtar et al. - 2018 - Defense Against Universal Adversarial Perturbation.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/MP4WD7ZN/Akhtar et al. - 2018 - Defense Against Universal Adversarial Perturbation.pdf:application/pdf}
}

@article{van_der_walt_numpy_2011,
	title = {The {NumPy} {Array}: {A} {Structure} for {Efficient} {Numerical} {Computation}},
	volume = {13},
	issn = {1521-9615},
	shorttitle = {The {NumPy} {Array}},
	url = {http://ieeexplore.ieee.org/document/5725236/},
	doi = {10.1109/MCSE.2011.37},
	number = {2},
	urldate = {2020-01-03},
	journal = {Computing in Science \& Engineering},
	author = {van der Walt, Stéfan and Colbert, S Chris and Varoquaux, Gaël},
	month = mar,
	year = {2011},
	pages = {22--30},
	file = {Submitted Version:/Users/maumau/Zotero/storage/K4DBYVRW/van der Walt et al. - 2011 - The NumPy Array A Structure for Efficient Numeric.pdf:application/pdf}
}

@article{kurita_towards_2019,
	title = {Towards {Robust} {Toxic} {Content} {Classification}},
	url = {http://arxiv.org/abs/1912.06872},
	abstract = {Toxic content detection aims to identify content that can offend or harm its recipients. Automated classifiers of toxic content need to be robust against adversaries who deliberately try to bypass filters. We propose a method of generating realistic model-agnostic attacks using a lexicon of toxic tokens, which attempts to mislead toxicity classifiers by diluting the toxicity signal either by obfuscating toxic tokens through character-level perturbations, or by injecting non-toxic distractor tokens. We show that these realistic attacks reduce the detection recall of state-of-the-art neural toxicity detectors, including those using ELMo and BERT, by more than 50\% in some cases. We explore two approaches for defending against such attacks. First, we examine the effect of training on synthetically noised data. Second, we propose the Contextual Denoising Autoencoder (CDAE): a method for learning robust representations that uses character-level and contextual information to denoise perturbed tokens. We show that the two approaches are complementary, improving robustness to both character-level perturbations and distractors, recovering a considerable portion of the lost accuracy. Finally, we analyze the robustness characteristics of the most competitive methods and outline practical considerations for improving toxicity detectors.},
	urldate = {2020-01-14},
	journal = {arXiv:1912.06872 [cs]},
	author = {Kurita, Keita and Belova, Anna and Anastasopoulos, Antonios},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.06872},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: to appear at EDSMLS 2020},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/VQ4TWLNL/Kurita et al. - 2019 - Towards Robust Toxic Content Classification.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/PGP6UE26/1912.html:text/html}
}

@article{chapman-rounds_emap_2019,
	title = {{EMAP}: {Explanation} by {Minimal} {Adversarial} {Perturbation}},
	shorttitle = {{EMAP}},
	url = {http://arxiv.org/abs/1912.00872},
	abstract = {Modern instance-based model-agnostic explanation methods (LIME, SHAP, L2X) are of great use in data-heavy industries for model diagnostics, and for end-user explanations. These methods generally return either a weighting or subset of input features as an explanation of the classification of an instance. An alternative literature argues instead that counterfactual instances provide a more useable characterisation of a black box classifier's decisions. We present EMAP, a neural network based approach which returns as Explanation the Minimal Adversarial Perturbation to an instance required to cause the underlying black box model to missclassify. We show that this approach combines the two paradigms, recovering the output of feature-weighting methods in continuous feature spaces, whilst also indicating the direction in which the nearest counterfactuals can be found. Our method also provides an implicit confidence estimate in its own explanations, adding a clarity to model diagnostics other methods lack. Additionally, EMAP improves upon the speed of sampling-based methods such as LIME by an order of magnitude, allowing for model explanations in time-critical applications, or at the dataset level, where sampling-based methods are infeasible. We extend our approach to categorical features using a partitioned Gumbel layer, and demonstrate its efficacy on several standard datasets.},
	urldate = {2020-01-14},
	journal = {arXiv:1912.00872 [cs, stat]},
	author = {Chapman-Rounds, Matt and Schulz, Marc-Andre and Pazos, Erik and Georgatzis, Konstantinos},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.00872},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 9 pages, 4 figures, 1 table},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/AFWJLYDD/Chapman-Rounds et al. - 2019 - EMAP Explanation by Minimal Adversarial Perturbat.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/C69PFBBC/1912.html:text/html}
}

@article{kodwani_initial_2019,
	title = {Initial conditions of the universe: {Decaying} tensor modes},
	shorttitle = {Initial conditions of the universe},
	url = {http://arxiv.org/abs/1910.01416},
	abstract = {Many models of the early universe predict that there should be primordial tensor perturbations. These leave an imprint into the temperature and polarisation anisotropies of the cosmic microwave background (CMB). The differential equation describing the primordial tensor perturbations is a second order differential equation and thus has two solutions. Canonically, the decaying solution of this equation in radiation domination is dropped as it diverges at early times and on superhorizon scales while it is then suppressed at late times. Furthermore, if there is an inflationary phase prior to the radiation domination phase, the amplitude of the decaying mode will also be highly suppressed as it enters the radiation phase, thus its effect will be negligible. In this study we remain agnostic to the early universe models describing pre-radiation domination physics and allow this mode to be present and see what effect it has on the CMB anisotropies. We find that the decaying mode, if normalised at the same time on subhorizon scales as the growing mode leaves an imprint on the CMB anisotropies that is identical to the growing mode. Contrary to expectation, on large scales both modes are poorly constrained for a scale invariant spectrum, and the apparent divergence of the decaying mode does not lead to a divergent physical observable. Quantitatively, the decaying mode can be more constrained both from temperature and polarisation anisotropies. We use a model independent, non-parametric, approach to constrain both of these primordial tensor perturbations using the temperature and polarisation anisotropies. We find that both modes are best constrained at the reionisation and recombination bumps and crucially, at the reionisation bump the decaying mode can be distinguished from the growing mode.},
	urldate = {2020-01-14},
	journal = {arXiv:1910.01416 [astro-ph, physics:gr-qc, physics:hep-th]},
	author = {Kodwani, Darsh and Meerburg, P. Daniel and Pen, Ue-Li and Wang, Xin},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.01416},
	keywords = {General Relativity and Quantum Cosmology, Astrophysics - Cosmology and Nongalactic Astrophysics, High Energy Physics - Theory},
	annote = {Comment: 10 pages, comments welcome!},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/AMBFGFF9/Kodwani et al. - 2019 - Initial conditions of the universe Decaying tenso.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/L22BQXDZ/1910.html:text/html}
}

@article{niu_automatically_2019,
	title = {Automatically {Learning} {Data} {Augmentation} {Policies} for {Dialogue} {Tasks}},
	url = {http://arxiv.org/abs/1909.12868},
	abstract = {Automatic data augmentation (AutoAugment) (Cubuk et al., 2019) searches for optimal perturbation policies via a controller trained using performance rewards of a sampled policy on the target task, hence reducing data-level model bias. While being a powerful algorithm, their work has focused on computer vision tasks, where it is comparatively easy to apply imperceptible perturbations without changing an image's semantic meaning. In our work, we adapt AutoAugment to automatically discover effective perturbation policies for natural language processing (NLP) tasks such as dialogue generation. We start with a pool of atomic operations that apply subtle semantic-preserving perturbations to the source inputs of a dialogue task (e.g., different POS-tag types of stopword dropout, grammatical errors, and paraphrasing). Next, we allow the controller to learn more complex augmentation policies by searching over the space of the various combinations of these atomic operations. Moreover, we also explore conditioning the controller on the source inputs of the target task, since certain strategies may not apply to inputs that do not contain that strategy's required linguistic features. Empirically, we demonstrate that both our input-agnostic and input-aware controllers discover useful data augmentation policies, and achieve significant improvements over the previous state-of-the-art, including trained on manually-designed policies.},
	urldate = {2020-01-14},
	journal = {arXiv:1909.12868 [cs]},
	author = {Niu, Tong and Bansal, Mohit},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.12868},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 7 pages (EMNLP 2019)},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/F48EU5VV/Niu and Bansal - 2019 - Automatically Learning Data Augmentation Policies .pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/4BQCARWU/1909.html:text/html}
}

@article{shankaranarayana_alime_2019,
	title = {{ALIME}: {Autoencoder} {Based} {Approach} for {Local} {Interpretability}},
	shorttitle = {{ALIME}},
	url = {http://arxiv.org/abs/1909.02437},
	abstract = {Machine learning and especially deep learning have garneredtremendous popularity in recent years due to their increased performanceover other methods. The availability of large amount of data has aidedin the progress of deep learning. Nevertheless, deep learning models areopaque and often seen as black boxes. Thus, there is an inherent need tomake the models interpretable, especially so in the medical domain. Inthis work, we propose a locally interpretable method, which is inspiredby one of the recent tools that has gained a lot of interest, called localinterpretable model-agnostic explanations (LIME). LIME generates singleinstance level explanation by artificially generating a dataset aroundthe instance (by randomly sampling and using perturbations) and thentraining a local linear interpretable model. One of the major issues inLIME is the instability in the generated explanation, which is caused dueto the randomly generated dataset. Another issue in these kind of localinterpretable models is the local fidelity. We propose novel modificationsto LIME by employing an autoencoder, which serves as a better weightingfunction for the local model. We perform extensive comparisons withdifferent datasets and show that our proposed method results in bothimproved stability, as well as local fidelity.},
	urldate = {2020-01-14},
	journal = {arXiv:1909.02437 [cs, stat]},
	author = {Shankaranarayana, Sharath M. and Runje, Davor},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.02437},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/H49Q289W/Shankaranarayana and Runje - 2019 - ALIME Autoencoder Based Approach for Local Interp.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/SQWGWCC9/1909.html:text/html}
}

@article{rothfuss_noise_2019,
	title = {Noise {Regularization} for {Conditional} {Density} {Estimation}},
	url = {http://arxiv.org/abs/1907.08982},
	abstract = {Modelling statistical relationships beyond the conditional mean is crucial in many settings. Conditional density estimation (CDE) aims to learn the full conditional probability density from data. Though highly expressive, neural network based CDE models can suffer from severe over-fitting when trained with the maximum likelihood objective. Due to the inherent structure of such models, classical regularization approaches in the parameter space are rendered ineffective. To address this issue, we develop a model-agnostic noise regularization method for CDE that adds random perturbations to the data during training. We demonstrate that the proposed approach corresponds to a smoothness regularization and prove its asymptotic consistency. In our experiments, noise regularization significantly and consistently outperforms other regularization methods across seven data sets and three CDE models. The effectiveness of noise regularization makes neural network based CDE the preferable method over previous non- and semi-parametric approaches, even when training data is scarce.},
	urldate = {2020-01-14},
	journal = {arXiv:1907.08982 [cs, stat]},
	author = {Rothfuss, Jonas and Ferreira, Fabio and Boehm, Simon and Walther, Simon and Ulrich, Maxim and Asfour, Tamim and Krause, Andreas},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.08982},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/GPMIPGLT/Rothfuss et al. - 2019 - Noise Regularization for Conditional Density Estim.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/PITTXFKF/1907.html:text/html}
}

@article{chudaykin_measuring_2019,
	title = {Measuring neutrino masses with large-scale structure: {Euclid} forecast with controlled theoretical error},
	volume = {2019},
	issn = {1475-7516},
	shorttitle = {Measuring neutrino masses with large-scale structure},
	url = {http://arxiv.org/abs/1907.06666},
	doi = {10.1088/1475-7516/2019/11/034},
	abstract = {We present a Markov-Chain Monte-Carlo (MCMC) forecast for the precision of neutrino mass and cosmological parameter measurements with a Euclid-like galaxy clustering survey. We use a complete perturbation theory model for the galaxy one-loop power spectrum and tree-level bispectrum, which includes bias, redshift space distortions, IR resummation for baryon acoustic oscillations and UV counterterms. The latter encapsulate various effects of short-scale dynamics which cannot be modeled within perturbation theory. Our MCMC procedure consistently computes the non-linear power spectra and bispectra as we scan over different cosmologies. The second ingredient of our approach is the theoretical error covariance which captures uncertainties due to higher-order non-linear corrections omitted in our model. Having specified characteristics of a Euclid-like spectroscopic survey, we generate and fit mock galaxy power spectrum and bispectrum likelihoods. Our results suggest that even under very agnostic assumptions about non-linearities and short-scale physics a future Euclid-like survey will be able to measure the sum of neutrino masses with a standard deviation of 28 meV. When combined with the Planck cosmic microwave background likelihood, this uncertainty decreases to 13 meV. Over-optimistically reducing the theoretical error on the bispectrum down to the two-loop level marginally tightens this bound to 11 meV. Moreover, we show that the future large-scale structure (LSS) spectroscopic data will greatly improve constraints on the other cosmological parameters, e.g. reaching a percent (per mille) error on the Hubble constant with LSS alone (LSS + Planck).},
	number = {11},
	urldate = {2020-01-14},
	journal = {Journal of Cosmology and Astroparticle Physics},
	author = {Chudaykin, Anton and Ivanov, Mikhail M.},
	month = nov,
	year = {2019},
	note = {arXiv: 1907.06666},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, High Energy Physics - Phenomenology},
	pages = {034--034},
	annote = {Comment: v1: 50 pages, 5 figures. v2: 62 pages, 7 figures, appendices D, E and F are added, appendix C is complemented by a new analysis for the optimally reconstructed BAO},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/N73EWS42/Chudaykin and Ivanov - 2019 - Measuring neutrino masses with large-scale structu.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/AINHVWBT/1907.html:text/html}
}

@article{lucic_explaining_2019,
	title = {Explaining {Predictions} from {Tree}-based {Boosting} {Ensembles}},
	url = {http://arxiv.org/abs/1907.02582},
	abstract = {Understanding how "black-box" models arrive at their predictions has sparked significant interest from both within and outside the AI community. Our work focuses on doing this by generating local explanations about individual predictions for tree-based ensembles, specifically Gradient Boosting Decision Trees (GBDTs). Given a correctly predicted instance in the training set, we wish to generate a counterfactual explanation for this instance, that is, the minimal perturbation of this instance such that the prediction flips to the opposite class. Most existing methods for counterfactual explanations are (1) model-agnostic, so they do not take into account the structure of the original model, and/or (2) involve building a surrogate model on top of the original model, which is not guaranteed to represent the original model accurately. There exists a method specifically for random forests; we wish to extend this method for GBDTs. This involves accounting for (1) the sequential dependency between trees and (2) training on the negative gradients instead of the original labels.},
	urldate = {2020-01-14},
	journal = {arXiv:1907.02582 [cs, stat]},
	author = {Lucic, Ana and Haned, Hinda and de Rijke, Maarten},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.02582},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
	annote = {Comment: SIGIR 2019: FACTS-IR Workshop},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/AVBXIU2T/Lucic et al. - 2019 - Explaining Predictions from Tree-based Boosting En.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/KCR8X3LJ/1907.html:text/html}
}

@article{zafar_dlime_2019,
	title = {{DLIME}: {A} {Deterministic} {Local} {Interpretable} {Model}-{Agnostic} {Explanations} {Approach} for {Computer}-{Aided} {Diagnosis} {Systems}},
	shorttitle = {{DLIME}},
	url = {http://arxiv.org/abs/1906.10263},
	abstract = {Local Interpretable Model-Agnostic Explanations (LIME) is a popular technique used to increase the interpretability and explainability of black box Machine Learning (ML) algorithms. LIME typically generates an explanation for a single prediction by any ML model by learning a simpler interpretable model (e.g. linear classifier) around the prediction through generating simulated data around the instance by random perturbation, and obtaining feature importance through applying some form of feature selection. While LIME and similar local algorithms have gained popularity due to their simplicity, the random perturbation and feature selection methods result in "instability" in the generated explanations, where for the same prediction, different explanations can be generated. This is a critical issue that can prevent deployment of LIME in a Computer-Aided Diagnosis (CAD) system, where stability is of utmost importance to earn the trust of medical professionals. In this paper, we propose a deterministic version of LIME. Instead of random perturbation, we utilize agglomerative Hierarchical Clustering (HC) to group the training data together and K-Nearest Neighbour (KNN) to select the relevant cluster of the new instance that is being explained. After finding the relevant cluster, a linear model is trained over the selected cluster to generate the explanations. Experimental results on three different medical datasets show the superiority for Deterministic Local Interpretable Model-Agnostic Explanations (DLIME), where we quantitatively determine the stability of DLIME compared to LIME utilizing the Jaccard similarity among multiple generated explanations.},
	urldate = {2020-01-14},
	journal = {arXiv:1906.10263 [cs, stat]},
	author = {Zafar, Muhammad Rehman and Khan, Naimul Mefraz},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.10263},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/IFNCHIBY/Zafar and Khan - 2019 - DLIME A Deterministic Local Interpretable Model-A.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/DFEA72SQ/1906.html:text/html}
}

@article{glampedakis_eikonal_2019,
	title = {Eikonal quasinormal modes of black holes beyond {General} {Relativity}},
	volume = {100},
	issn = {2470-0010, 2470-0029},
	url = {http://arxiv.org/abs/1906.05455},
	doi = {10.1103/PhysRevD.100.044040},
	abstract = {Much of our physical intuition about black hole quasinormal modes in general relativity comes from the eikonal/geometric optics approximation. According to the well-established eikonal model, the fundamental quasinormal mode represents wavepackets orbiting in the vicinity of the black hole's geodesic photon ring, slowly peeling off towards the event horizon and infinity. Besides its strength as a "visualisation" tool, the eikonal approximation also provides a simple quantitative method for calculating the mode frequency, in close agreement with rigorous numerical results. In this paper we move away from Einstein's theory and its garden-variety black holes and go on to consider spherically symmetric black holes in modified theories of gravity through the lens of the eikonal approximation. The quasinormal modes of such black holes are typically described by a set of coupled wave equations for the various field degrees of freedom. Considering a general, theory-agnostic, system of two equations for two perturbed fields, we derive eikonal formulae for the complex fundamental quasinormal mode frequency. In addition we show that the eikonal modes can be related to the extremum of an effective potential and its associated "photon ring". As an application of our results we consider a specific example of a modified theory of gravity with known black hole quasinormal modes and find that these are well approximated by the eikonal formulae.},
	number = {4},
	urldate = {2020-01-14},
	journal = {Physical Review D},
	author = {Glampedakis, Kostas and Silva, Hector O.},
	month = aug,
	year = {2019},
	note = {arXiv: 1906.05455},
	keywords = {General Relativity and Quantum Cosmology, High Energy Physics - Theory, Astrophysics - High Energy Astrophysical Phenomena},
	pages = {044040},
	annote = {Comment: 12 pages, 1 figure. Updated to match the published version and it includes a more rigorous implementation of the eikonal approximation},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/BAB3L3YS/Glampedakis and Silva - 2019 - Eikonal quasinormal modes of black holes beyond Ge.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/GS95QURD/1906.html:text/html}
}

@article{naseer_cross-domain_2019,
	title = {Cross-{Domain} {Transferability} of {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1905.11736},
	abstract = {Adversarial examples reveal the blind spots of deep neural networks (DNNs) and represent a major concern for security-critical applications. The transferability of adversarial examples makes real-world attacks possible in black-box settings, where the attacker is forbidden to access the internal parameters of the model. The underlying assumption in most adversary generation methods, whether learning an instance-specific or an instance-agnostic perturbation, is the direct or indirect reliance on the original domain-specific data distribution. In this work, for the first time, we demonstrate the existence of domain-invariant adversaries, thereby showing common adversarial space among different datasets and models. To this end, we propose a framework capable of launching highly transferable attacks that crafts adversarial patterns to mislead networks trained on wholly different domains. For instance, an adversarial function learned on Paintings, Cartoons or Medical images can successfully perturb ImageNet samples to fool the classifier, with success rates as high as \${\textbackslash}sim\$99{\textbackslash}\% (\${\textbackslash}ell\_\{{\textbackslash}infty\} {\textbackslash}le 10\$). The core of our proposed adversarial function is a generative network that is trained using a relativistic supervisory signal that enables domain-invariant perturbations. Our approach sets the new state-of-the-art for fooling rates, both under the white-box and black-box scenarios. Furthermore, despite being an instance-agnostic perturbation function, our attack outperforms the conventionally much stronger instance-specific attack methods.},
	urldate = {2020-01-14},
	journal = {arXiv:1905.11736 [cs]},
	author = {Naseer, Muzammal and Khan, Salman H. and Khan, Harris and Khan, Fahad Shahbaz and Porikli, Fatih},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.11736},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at NeurIPS 2019 (Camera Ready). Source Code along with pretrained adversarial generators is available at https://github.com/Muzammal-Naseer/Cross-domain-perturbations},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/VBK9PV9T/Naseer et al. - 2019 - Cross-Domain Transferability of Adversarial Pertur.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/S4FP9CNW/1905.html:text/html}
}

@article{bose_generalizable_2019,
	title = {Generalizable {Adversarial} {Attacks} {Using} {Generative} {Models}},
	url = {http://arxiv.org/abs/1905.10864},
	abstract = {Adversarial attacks on deep neural networks traditionally rely on a constrained optimization paradigm, where an optimization procedure is used to obtain a single adversarial perturbation for a given input example. Here, we instead view adversarial attacks as a generative modelling problem, with the goal of producing entire distributions of adversarial examples given an unperturbed input. We show that this generative perspective can be used to design a unified encoder-decoder framework, which is domain-agnostic in that the same framework can be employed to attack different domains with minimal modification. Across three diverse domains---images, text, and graphs---our approach generates whitebox attacks with success rates that are competitive with or superior to existing approaches, with a new state-of-the-art achieved in the graph domain. Finally, we demonstrate that our generative framework can efficiently generate a diverse set of attacks for a single given input, and is even capable of attacking unseen test instances in a zero-shot manner, exhibiting attack generalization.},
	urldate = {2020-01-14},
	journal = {arXiv:1905.10864 [cs, stat]},
	author = {Bose, Avishek Joey and Cianflone, Andre and Hamilton, William L.},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.10864},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/PSWA62R5/Bose et al. - 2019 - Generalizable Adversarial Attacks Using Generative.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/JRTYD5EB/1905.html:text/html}
}

@article{neekhara_universal_2019,
	title = {Universal {Adversarial} {Perturbations} for {Speech} {Recognition} {Systems}},
	url = {http://arxiv.org/abs/1905.03828},
	abstract = {In this work, we demonstrate the existence of universal adversarial audio perturbations that cause mis-transcription of audio signals by automatic speech recognition (ASR) systems. We propose an algorithm to find a single quasi-imperceptible perturbation, which when added to any arbitrary speech signal, will most likely fool the victim speech recognition model. Our experiments demonstrate the application of our proposed technique by crafting audio-agnostic universal perturbations for the state-of-the-art ASR system -- Mozilla DeepSpeech. Additionally, we show that such perturbations generalize to a significant extent across models that are not available during training, by performing a transferability test on a WaveNet based ASR system.},
	urldate = {2020-01-14},
	journal = {arXiv:1905.03828 [cs, eess, stat]},
	author = {Neekhara, Paarth and Hussain, Shehzeen and Pandey, Prakhar and Dubnov, Shlomo and McAuley, Julian and Koushanfar, Farinaz},
	month = aug,
	year = {2019},
	note = {arXiv: 1905.03828},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Published as a conference paper at INTERSPEECH 2019},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/UVJDRTEQ/Neekhara et al. - 2019 - Universal Adversarial Perturbations for Speech Rec.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/AEDW4RH9/1905.html:text/html}
}

@article{abdullah_practical_2019,
	title = {Practical {Hidden} {Voice} {Attacks} against {Speech} and {Speaker} {Recognition} {Systems}},
	url = {http://arxiv.org/abs/1904.05734},
	abstract = {Voice Processing Systems (VPSes), now widely deployed, have been made significantly more accurate through the application of recent advances in machine learning. However, adversarial machine learning has similarly advanced and has been used to demonstrate that VPSes are vulnerable to the injection of hidden commands - audio obscured by noise that is correctly recognized by a VPS but not by human beings. Such attacks, though, are often highly dependent on white-box knowledge of a specific machine learning model and limited to specific microphones and speakers, making their use across different acoustic hardware platforms (and thus their practicality) limited. In this paper, we break these dependencies and make hidden command attacks more practical through model-agnostic (blackbox) attacks, which exploit knowledge of the signal processing algorithms commonly used by VPSes to generate the data fed into machine learning systems. Specifically, we exploit the fact that multiple source audio samples have similar feature vectors when transformed by acoustic feature extraction algorithms (e.g., FFTs). We develop four classes of perturbations that create unintelligible audio and test them against 12 machine learning models, including 7 proprietary models (e.g., Google Speech API, Bing Speech API, IBM Speech API, Azure Speaker API, etc), and demonstrate successful attacks against all targets. Moreover, we successfully use our maliciously generated audio samples in multiple hardware configurations, demonstrating effectiveness across both models and real systems. In so doing, we demonstrate that domain-specific knowledge of audio signal processing represents a practical means of generating successful hidden voice command attacks.},
	urldate = {2020-01-14},
	journal = {arXiv:1904.05734 [cs, eess]},
	author = {Abdullah, Hadi and Garcia, Washington and Peeters, Christian and Traynor, Patrick and Butler, Kevin R. B. and Wilson, Joseph},
	month = mar,
	year = {2019},
	note = {arXiv: 1904.05734},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/R5W82BNV/Abdullah et al. - 2019 - Practical Hidden Voice Attacks against Speech and .pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/XLAMNGV2/1904.html:text/html}
}

@article{obeirne_constraining_2019,
	title = {Constraining alternative polarization states of gravitational waves from individual black hole binaries using pulsar timing arrays},
	volume = {99},
	issn = {2470-0010, 2470-0029},
	url = {http://arxiv.org/abs/1904.02744},
	doi = {10.1103/PhysRevD.99.124039},
	abstract = {Pulsar timing arrays are sensitive to gravitational wave perturbations produced by individual supermassive black hole binaries during their early inspiral phase. Modified gravity theories allow for the emission of gravitational dipole radiation, which is enhanced relative to the quadrupole contribution for low orbital velocities, making the early inspiral an ideal regime to test for the presence of modified gravity effects. Using a theory-agnostic description of modified gravity theories based on the parametrized post-Einsteinian framework, we explore the possibility of detecting deviations from General Relativity using simulated pulsar timing array data, and provide forecasts for the constraints that can be achieved. We generalize the \{{\textbackslash}tt enterprise\} pulsar timing software to account for possible additional polarization states and modifications to the phase evolution, and study how accurately the parameters of simulated signals can be recovered. We find that while a pure dipole model can partially recover a pure quadrupole signal, there is little possibility for confusion when the full model with all polarization states is used. With no signal present, and using noise levels comparable to those seen in contemporary arrays, we produce forecasts for the upper limits that can be placed on the amplitudes of alternative polarization modes as a function of the sky location of the source.},
	number = {12},
	urldate = {2020-01-14},
	journal = {Physical Review D},
	author = {O'Beirne, Logan and Cornish, Neil J. and Vigeland, Sarah J. and Taylor, Stephen R.},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.02744},
	keywords = {General Relativity and Quantum Cosmology, Astrophysics - High Energy Astrophysical Phenomena},
	pages = {124039},
	annote = {Comment: 12 pages, 8 figures},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/LAX9R7EC/O'Beirne et al. - 2019 - Constraining alternative polarization states of gr.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/GDBZCTMB/1904.html:text/html}
}

@article{carullo_observational_2019,
	title = {Observational {Black} {Hole} {Spectroscopy}: {A} time-domain multimode analysis of {GW150914}},
	volume = {99},
	issn = {2470-0010, 2470-0029},
	shorttitle = {Observational {Black} {Hole} {Spectroscopy}},
	url = {http://arxiv.org/abs/1902.07527},
	doi = {10.1103/PhysRevD.99.123029},
	abstract = {The detection of the least damped quasi-normal mode from the remnant of the gravitational wave event GW150914 realised the long sought possibility to observationally study the properties of quasi-stationary black hole spacetimes through gravitational waves. Past literature has extensively explored this possibility and the emerging field has been named "black hole spectroscopy". In this study, we present results regarding the ringdown spectrum of GW150914, obtained by application of Bayesian inference to identify and characterise the ringdown modes. We employ a pure time-domain analysis method which infers from the data the time of transition between the non-linear and quasi-linear regime of the post-merger emission in concert with all other parameters characterising the source. We find that the data provides no evidence for the presence of more than one quasi-normal mode. However, from the central frequency and damping time posteriors alone, no unambiguous identification of a single mode is possible. More in-depth analysis adopting a ringdown model based on results in perturbation theory over the Kerr metric, confirms that the data do not provide enough evidence to discriminate among an \$l=2\$ and the \$l=3\$ subset of modes. Our work provides the first comprehensive agnostic framework to observationally investigate astrophysical black holes' ringdown spectra.},
	number = {12},
	urldate = {2020-01-14},
	journal = {Physical Review D},
	author = {Carullo, Gregorio and Del Pozzo, Walter and Veitch, John},
	month = jun,
	year = {2019},
	note = {arXiv: 1902.07527},
	keywords = {General Relativity and Quantum Cosmology},
	pages = {123029},
	annote = {Comment: 9 pages, 8 figures},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/RC5USVIF/Carullo et al. - 2019 - Observational Black Hole Spectroscopy A time-doma.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/MCAV29YS/1902.html:text/html}
}

@article{kommajosula_shape-design_2019,
	title = {Shape-design for stabilizing micro-particles in inertial microfluidic flows},
	url = {http://arxiv.org/abs/1902.05935},
	abstract = {Design of microparticles which stabilize at the centerline of a channel flow when part of a dilute suspension is examined numerically for moderate Reynolds numbers (\$10 {\textbackslash}le Re {\textbackslash}le 80\$). Stability metrics for particles with arbitrary shapes are formulated based on linear-stability theory. Particle shape is parametrized by a compact, Non-Uniform Rational B-Spline (NURBS)-based representation. Shape-design is posed as an optimization problem and solved using adaptive Bayesian optimization. We focus on designing particles for maximal stability at the channel-centerline robust to perturbations. Our results indicate that centerline-focusing particles are families of characteristic "fish"/"bottle"/"dumbbell"-like shapes, exhibiting fore-aft asymmetry. A parametric exploration is then performed to identify stable particle-designs at different k's (particle chord-to-channel width ratio) and Re's (\$0.1 {\textbackslash}le k {\textbackslash}le 0.4, 10 {\textbackslash}le Re {\textbackslash}le 80\$). Particles at high-k's and Re's are highly stabilized when compared to those at low-k's and Re's. A comparison of the modified dumbbell designs from the current framework also shows better performance to perturbations in Fluid-Structure Interaction (FSI) when compared to the rod-disk model reported previously (Uspal \& Doyle 2014) for low-Re Hele-Shaw flow. We identify basins of attraction around the centerline, which span larger release-angle-ranges and lateral locations (tending to the channel width) for narrower channels, which effectively standardizes the notion of global focusing in such configurations using the current stability-paradigm. The present framework is illustrated for 2D cases and is potentially generalizable to stability in 3D flow-fields. The current formulation is also agnostic to Re and particle/channel geometry which indicates substantial potential for integration with imaging flow-cytometry tools and microfluidic biosensing-assays.},
	urldate = {2020-01-14},
	journal = {arXiv:1902.05935 [physics]},
	author = {Kommajosula, Aditya and Stoecklein, Daniel and Di Carlo, Dino and Ganapathysubramanian, Baskar},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.05935},
	keywords = {Physics - Fluid Dynamics},
	annote = {Comment: 27 pages, 18 figures, modified the LaTeX document template, corrected typos},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/BLL992H5/Kommajosula et al. - 2019 - Shape-design for stabilizing micro-particles in in.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/AT8W6RIE/1902.html:text/html}
}

@article{li_universal_2019,
	title = {Universal {Rules} for {Fooling} {Deep} {Neural} {Networks} based {Text} {Classification}},
	url = {http://arxiv.org/abs/1901.07132},
	abstract = {Recently, deep learning based natural language processing techniques are being extensively used to deal with spam mail, censorship evaluation in social networks, among others. However, there is only a couple of works evaluating the vulnerabilities of such deep neural networks. Here, we go beyond attacks to investigate, for the first time, universal rules, i.e., rules that are sample agnostic and therefore could turn any text sample in an adversarial one. In fact, the universal rules do not use any information from the method itself (no information from the method, gradient information or training dataset information is used), making them black-box universal attacks. In other words, the universal rules are sample and method agnostic. By proposing a coevolutionary optimization algorithm we show that it is possible to create universal rules that can automatically craft imperceptible adversarial samples (only less than five perturbations which are close to misspelling are inserted in the text sample). A comparison with a random search algorithm further justifies the strength of the method. Thus, universal rules for fooling networks are here shown to exist. Hopefully, the results from this work will impact the development of yet more sample and model agnostic attacks as well as their defenses, culminating in perhaps a new age for artificial intelligence.},
	urldate = {2020-01-14},
	journal = {arXiv:1901.07132 [cs, stat]},
	author = {Li, Di and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	month = apr,
	year = {2019},
	note = {arXiv: 1901.07132},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/DPEXDEMP/Li et al. - 2019 - Universal Rules for Fooling Deep Neural Networks b.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/WHHI73LJ/1901.html:text/html}
}

@article{mustafa_image_2020,
	title = {Image {Super}-{Resolution} as a {Defense} {Against} {Adversarial} {Attacks}},
	volume = {29},
	issn = {1057-7149, 1941-0042},
	url = {http://arxiv.org/abs/1901.01677},
	doi = {10.1109/TIP.2019.2940533},
	abstract = {Convolutional Neural Networks have achieved significant success across multiple computer vision tasks. However, they are vulnerable to carefully crafted, human-imperceptible adversarial noise patterns which constrain their deployment in critical security-sensitive systems. This paper proposes a computationally efficient image enhancement approach that provides a strong defense mechanism to effectively mitigate the effect of such adversarial perturbations. We show that deep image restoration networks learn mapping functions that can bring off-the-manifold adversarial samples onto the natural image manifold, thus restoring classification towards correct classes. A distinguishing feature of our approach is that, in addition to providing robustness against attacks, it simultaneously enhances image quality and retains models performance on clean images. Furthermore, the proposed method does not modify the classifier or requires a separate mechanism to detect adversarial images. The effectiveness of the scheme has been demonstrated through extensive experiments, where it has proven a strong defense in gray-box settings. The proposed scheme is simple and has the following advantages: (1) it does not require any model training or parameter optimization, (2) it complements other existing defense mechanisms, (3) it is agnostic to the attacked model and attack type and (4) it provides superior performance across all popular attack algorithms. Our codes are publicly available at https://github.com/aamir-mustafa/super-resolution-adversarial-defense.},
	urldate = {2020-01-14},
	journal = {IEEE Transactions on Image Processing},
	author = {Mustafa, Aamir and Khan, Salman H. and Hayat, Munawar and Shen, Jianbing and Shao, Ling},
	year = {2020},
	note = {arXiv: 1901.01677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1711--1724},
	annote = {Comment: Published in IEEE Transactions in Image Processing},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/QVHWUJ6E/Mustafa et al. - 2020 - Image Super-Resolution as a Defense Against Advers.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/J8RKZPRV/1901.html:text/html}
}

@article{hogan_universal_2018,
	title = {Universal {Decision}-{Based} {Black}-{Box} {Perturbations}: {Breaking} {Security}-{Through}-{Obscurity} {Defenses}},
	shorttitle = {Universal {Decision}-{Based} {Black}-{Box} {Perturbations}},
	url = {http://arxiv.org/abs/1811.03733},
	abstract = {We study the problem of finding a universal (image-agnostic) perturbation to fool machine learning (ML) classifiers (e.g., neural nets, decision tress) in the hard-label black-box setting. Recent work in adversarial ML in the white-box setting (model parameters are known) has shown that many state-of-the-art image classifiers are vulnerable to universal adversarial perturbations: a fixed human-imperceptible perturbation that, when added to any image, causes it to be misclassified with high probability Kurakin et al. [2016], Szegedy et al. [2013], Chen et al. [2017a], Carlini and Wagner [2017]. This paper considers a more practical and challenging problem of finding such universal perturbations in an obscure (or black-box) setting. More specifically, we use zeroth order optimization algorithms to find such a universal adversarial perturbation when no model information is revealed-except that the attacker can make queries to probe the classifier. We further relax the assumption that the output of a query is continuous valued confidence scores for all the classes and consider the case where the output is a hard-label decision. Surprisingly, we found that even in these extremely obscure regimes, state-of-the-art ML classifiers can be fooled with a very high probability just by adding a single human-imperceptible image perturbation to any natural image. The surprising existence of universal perturbations in a hard-label black-box setting raises serious security concerns with the existence of a universal noise vector that adversaries can possibly exploit to break a classifier on most natural images.},
	urldate = {2020-01-14},
	journal = {arXiv:1811.03733 [cs, stat]},
	author = {Hogan, Thomas A. and Kailkhura, Bhavya},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.03733},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/J92KXNGG/Hogan and Kailkhura - 2018 - Universal Decision-Based Black-Box Perturbations .pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/NLXWBY3L/1811.html:text/html}
}

@article{mopuri_ask_2018,
	title = {Ask, {Acquire}, and {Attack}: {Data}-free {UAP} {Generation} using {Class} {Impressions}},
	shorttitle = {Ask, {Acquire}, and {Attack}},
	url = {http://arxiv.org/abs/1808.01153},
	abstract = {Deep learning models are susceptible to input specific noise, called adversarial perturbations. Moreover, there exist input-agnostic noise, called Universal Adversarial Perturbations (UAP) that can affect inference of the models over most input samples. Given a model, there exist broadly two approaches to craft UAPs: (i) data-driven: that require data, and (ii) data-free: that do not require data samples. Data-driven approaches require actual samples from the underlying data distribution and craft UAPs with high success (fooling) rate. However, data-free approaches craft UAPs without utilizing any data samples and therefore result in lesser success rates. In this paper, for data-free scenarios, we propose a novel approach that emulates the effect of data samples with class impressions in order to craft UAPs using data-driven objectives. Class impression for a given pair of category and model is a generic representation (in the input space) of the samples belonging to that category. Further, we present a neural network based generative model that utilizes the acquired class impressions to learn crafting UAPs. Experimental evaluation demonstrates that the learned generative model, (i) readily crafts UAPs via simple feed-forwarding through neural network layers, and (ii) achieves state-of-the-art success rates for data-free scenario and closer to that for data-driven setting without actually utilizing any data samples.},
	urldate = {2020-01-14},
	journal = {arXiv:1808.01153 [cs]},
	author = {Mopuri, Konda Reddy and Uppala, Phani Krishna and Babu, R. Venkatesh},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.01153},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted in ECCV 2018},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/FIVUM2QY/Mopuri et al. - 2018 - Ask, Acquire, and Attack Data-free UAP Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/WZTRQFXY/1808.html:text/html}
}

@article{prakash_deflecting_2018,
	title = {Deflecting {Adversarial} {Attacks} with {Pixel} {Deflection}},
	url = {http://arxiv.org/abs/1801.08926},
	abstract = {CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classification accuracy is significantly preserved in the presence of such adversarial manipulations. Image classifiers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.},
	urldate = {2020-01-14},
	journal = {arXiv:1801.08926 [cs]},
	author = {Prakash, Aaditya and Moran, Nick and Garber, Solomon and DiLillo, Antonella and Storer, James},
	month = mar,
	year = {2018},
	note = {arXiv: 1801.08926},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	annote = {Comment: Accepted to IEEE CVPR 2018 as Spotlight},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/UGD52EIZ/Prakash et al. - 2018 - Deflecting Adversarial Attacks with Pixel Deflecti.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/QRP82MFW/1801.html:text/html}
}

@article{mopuri_generalizable_2018,
	title = {Generalizable {Data}-free {Objective} for {Crafting} {Universal} {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1801.08092},
	abstract = {Machine learning models are susceptible to adversarial perturbations: small changes to input that can cause large changes in output. It is also demonstrated that there exist input-agnostic perturbations, called universal adversarial perturbations, which can change the inference of target model on most of the data samples. However, existing methods to craft universal perturbations are (i) task specific, (ii) require samples from the training data distribution, and (iii) perform complex optimizations. Additionally, because of the data dependence, fooling ability of the crafted perturbations is proportional to the available training data. In this paper, we present a novel, generalizable and data-free approaches for crafting universal adversarial perturbations. Independent of the underlying task, our objective achieves fooling via corrupting the extracted features at multiple layers. Therefore, the proposed objective is generalizable to craft image-agnostic perturbations across multiple vision tasks such as object recognition, semantic segmentation, and depth estimation. In the practical setting of black-box attack scenario (when the attacker does not have access to the target model and it's training data), we show that our objective outperforms the data dependent objectives to fool the learned models. Further, via exploiting simple priors related to the data distribution, our objective remarkably boosts the fooling ability of the crafted perturbations. Significant fooling rates achieved by our objective emphasize that the current deep learning models are now at an increased risk, since our objective generalizes across multiple tasks without the requirement of training data for crafting the perturbations. To encourage reproducible research, we have released the codes for our proposed algorithm.},
	urldate = {2020-01-14},
	journal = {arXiv:1801.08092 [cs]},
	author = {Mopuri, Konda Reddy and Ganeshan, Aditya and Babu, R. Venkatesh},
	month = jul,
	year = {2018},
	note = {arXiv: 1801.08092},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: TPAMI {\textbar} Repository: https://github.com/val-iisc/GD-UAP},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/DHFCCLK8/Mopuri et al. - 2018 - Generalizable Data-free Objective for Crafting Uni.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/KBVT6NYV/1801.html:text/html}
}

@article{akhtar_defense_2018-1,
	title = {Defense against {Universal} {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1711.05929},
	abstract = {Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to `any' image can fool a state-of-the-art network classifier to change its prediction about the image label. These `Universal Adversarial Perturbations' pose a serious threat to the success of Deep Learning in practice. We present the first dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as `pre-input' layers to a targeted model, such that the targeted model needs no modification. The PRN is learned from real and synthetic image-agnostic perturbations, where an efficient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is first passed through the PRN and verified by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the network classifiers against unseen adversarial perturbations in the real-world scenarios with up to 97.5\% success rate. The PRN also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate.},
	urldate = {2020-01-14},
	journal = {arXiv:1711.05929 [cs]},
	author = {Akhtar, Naveed and Liu, Jian and Mian, Ajmal},
	month = feb,
	year = {2018},
	note = {arXiv: 1711.05929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted in IEEE CVPR 2018},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/WGX2PN2S/Akhtar et al. - 2018 - Defense against Universal Adversarial Perturbation.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/7Z8PD6VX/1711.html:text/html}
}

@article{song_pixeldefend_2018,
	title = {{PixelDefend}: {Leveraging} {Generative} {Models} to {Understand} and {Defend} against {Adversarial} {Examples}},
	shorttitle = {{PixelDefend}},
	url = {http://arxiv.org/abs/1710.10766},
	abstract = {Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63\% to 84\% for Fashion MNIST and from 32\% to 70\% for CIFAR-10.},
	urldate = {2020-01-14},
	journal = {arXiv:1710.10766 [cs]},
	author = {Song, Yang and Kim, Taesup and Nowozin, Sebastian and Ermon, Stefano and Kushman, Nate},
	month = may,
	year = {2018},
	note = {arXiv: 1710.10766},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR 2018},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/B7BMCK5W/Song et al. - 2018 - PixelDefend Leveraging Generative Models to Under.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/JCMHTAK6/1710.html:text/html}
}

@article{yadollahpour_exploring_2017,
	title = {Exploring and {Exploiting} {Diversity} for {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1709.01625},
	abstract = {Semantic image segmentation is an important computer vision task that is difficult because it consists of both recognition and segmentation. The task is often cast as a structured output problem on an exponentially large output-space, which is typically modeled by a discrete probabilistic model. The best segmentation is found by inferring the Maximum a-Posteriori (MAP) solution over the output distribution defined by the model. Due to limitations in optimization, the model cannot be arbitrarily complex. This leads to a trade-off: devise a more accurate model that incorporates rich high-order interactions between image elements at the cost of inaccurate and possibly intractable optimization OR leverage a tractable model which produces less accurate MAP solutions but may contain high quality solutions as other modes of its output distribution. This thesis investigates the latter and presents a two stage approach to semantic segmentation. In the first stage a tractable segmentation model outputs a set of high probability segmentations from the underlying distribution that are not just minor perturbations of each other. Critically the output of this stage is a diverse set of plausible solutions and not just a single one. In the second stage, a discriminatively trained re-ranking model selects the best segmentation from this set. The re-ranking stage can use much more complex features than what could be tractably used in the segmentation model, allowing a better exploration of the solution space than simply returning the MAP solution. The formulation is agnostic to the underlying segmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, which makes it applicable to a wide range of models and inference methods. Evaluation of the approach on a number of semantic image segmentation benchmark datasets highlight its superiority over inferring the MAP solution.},
	urldate = {2020-01-14},
	journal = {arXiv:1709.01625 [cs]},
	author = {Yadollahpour, Payman},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01625},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: PhD Thesis. For overall document size considerations the results in this appendix section have been moved to http://ttic.uchicago.edu/{\textasciitilde}pyadolla/papers/thesis.pdf},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/M5UH67PV/Yadollahpour - 2017 - Exploring and Exploiting Diversity for Image Segme.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/2LZEHN9C/1709.html:text/html}
}

@article{bearden_instantons_2018,
	title = {Instantons in self-organizing logic gates},
	volume = {9},
	issn = {2331-7019},
	url = {http://arxiv.org/abs/1708.08949},
	doi = {10.1103/PhysRevApplied.9.034029},
	abstract = {Self-organizing logic is a recently-suggested framework that allows the solution of Boolean truth tables "in reverse," i.e., it is able to satisfy the logical proposition of gates regardless to which terminal(s) the truth value is assigned ("terminal-agnostic logic"). It can be realized if time non-locality (memory) is present. A practical realization of self-organizing logic gates (SOLGs) can be done by combining circuit elements with and without memory. By employing one such realization, we show, numerically, that SOLGs exploit elementary instantons to reach equilibrium points. Instantons are classical trajectories of the non-linear equations of motion describing SOLGs, and connect topologically distinct critical points in the phase space. By linear analysis at those points, we show that these instantons connect the initial critical point of the dynamics, with at least one unstable direction, directly to the final fixed point. We also show that the memory content of these gates only affects the relaxation time to reach the logically consistent solution. Finally, we demonstrate, by solving the corresponding stochastic differential equations, that since instantons connect critical points, noise and perturbations may change the instanton trajectory in the phase space, but not the initial and final critical points. Therefore, even for extremely large noise levels, the gates self-organize to the correct solution. Our work provides a physical understanding of, and can serve as an inspiration for, new models of bi-directional logic gates that are emerging as important tools in physics-inspired, unconventional computing.},
	number = {3},
	urldate = {2020-01-14},
	journal = {Physical Review Applied},
	author = {Bearden, Sean R. B. and Manukian, Haik and Traversa, Fabio L. and Di Ventra, Massimiliano},
	month = mar,
	year = {2018},
	note = {arXiv: 1708.08949},
	keywords = {Computer Science - Emerging Technologies, Condensed Matter - Mesoscale and Nanoscale Physics, Mathematics - Dynamical Systems},
	pages = {034029},
	annote = {Comment: 7 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/ZH45N5ND/Bearden et al. - 2018 - Instantons in self-organizing logic gates.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/NKCX28TC/1708.html:text/html}
}

@article{papadopoulos_network_2018,
	title = {Network {Analysis} of {Particles} and {Grains}},
	volume = {6},
	issn = {2051-1310, 2051-1329},
	url = {http://arxiv.org/abs/1708.08080},
	doi = {10.1093/comnet/cny005},
	abstract = {The arrangements of particles and forces in granular materials have a complex organization on multiple spatial scales that ranges from local structures to mesoscale and system-wide ones. This multiscale organization can affect how a material responds or reconfigures when exposed to external perturbations or loading. The theoretical study of particle-level, force-chain, domain, and bulk properties requires the development and application of appropriate physical, mathematical, statistical, and computational frameworks. Traditionally, granular materials have been investigated using particulate or continuum models, each of which tends to be implicitly agnostic to multiscale organization. Recently, tools from network science have emerged as powerful approaches for probing and characterizing heterogeneous architectures across different scales in complex systems, and a diverse set of methods have yielded fascinating insights into granular materials. In this paper, we review work on network-based approaches to studying granular matter and explore the potential of such frameworks to provide a useful description of these systems and to enhance understanding of their underlying physics. We also outline a few open questions and highlight particularly promising future directions in the analysis and design of granular matter and other kinds of material networks.},
	number = {4},
	urldate = {2020-01-14},
	journal = {Journal of Complex Networks},
	author = {Papadopoulos, Lia and Porter, Mason A. and Daniels, Karen E. and Bassett, Danielle S.},
	month = aug,
	year = {2018},
	note = {arXiv: 1708.08080},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Soft Condensed Matter, Mathematics - Algebraic Topology, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
	pages = {485--565},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/IAPWZ36I/Papadopoulos et al. - 2018 - Network Analysis of Particles and Grains.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/B3AHLRJB/1708.html:text/html}
}

@article{singh_programs_2016,
	title = {Programs as {Black}-{Box} {Explanations}},
	url = {http://arxiv.org/abs/1611.07579},
	abstract = {Recent work in model-agnostic explanations of black-box machine learning has demonstrated that interpretability of complex models does not have to come at the cost of accuracy or model flexibility. However, it is not clear what kind of explanations, such as linear models, decision trees, and rule lists, are the appropriate family to consider, and different tasks and models may benefit from different kinds of explanations. Instead of picking a single family of representations, in this work we propose to use "programs" as model-agnostic explanations. We show that small programs can be expressive yet intuitive as explanations, and generalize over a number of existing interpretable families. We propose a prototype program induction method based on simulated annealing that approximates the local behavior of black-box classifiers around a specific prediction using random perturbations. Finally, we present preliminary application on small datasets and show that the generated explanations are intuitive and accurate for a number of classifiers.},
	urldate = {2020-01-14},
	journal = {arXiv:1611.07579 [cs, stat]},
	author = {Singh, Sameer and Ribeiro, Marco Tulio and Guestrin, Carlos},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.07579},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Presented at NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/RCKG4YHA/Singh et al. - 2016 - Programs as Black-Box Explanations.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/C7MYEMBV/1611.html:text/html}
}

@article{watson_approximate_2015,
	title = {Approximate {Models} and {Robust} {Decisions}},
	url = {http://arxiv.org/abs/1402.6118},
	abstract = {Decisions based partly or solely on predictions from probabilistic models may be sensitive to model misspecification. Statisticians are taught from an early stage that "all models are wrong", but little formal guidance exists on how to assess the impact of model approximation on decision making, or how to proceed when optimal actions appear sensitive to model fidelity. This article presents an overview of recent developments across different disciplines to address this. We review diagnostic techniques, including graphical approaches and summary statistics, to help highlight decisions made through minimised expected loss that are sensitive to model misspecification. We then consider formal methods for decision making under model misspecification by quantifying stability of optimal actions to perturbations to the model within a neighbourhood of model space. This neighbourhood is defined in either one of two ways. Firstly, in a strong sense via an information (Kullback-Leibler) divergence around the approximating model. Or using a nonparametric model extension, again centred at the approximating model, in order to `average out' over possible misspecifications. This is presented in the context of recent work in the robust control, macroeconomics and financial mathematics literature. We adopt a Bayesian approach throughout although the methods are agnostic to this position.},
	urldate = {2020-01-14},
	journal = {arXiv:1402.6118 [stat]},
	author = {Watson, James and Holmes, Chris},
	month = mar,
	year = {2015},
	note = {arXiv: 1402.6118},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/UHIQKFSN/Watson and Holmes - 2015 - Approximate Models and Robust Decisions.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/TX7T4BX5/1402.html:text/html}
}

@article{olson_large_2014,
	title = {Large eddy simulation requirements for the {Richtmyer}-{Meshkov} {Instability}},
	volume = {26},
	issn = {1070-6631, 1089-7666},
	url = {http://arxiv.org/abs/1402.2665},
	doi = {10.1063/1.4871396},
	abstract = {The shock induced mixing of two gases separated by a perturbed interface is investigated through Large Eddy Simulation (LES) and Direct Numerical Simulation (DNS). In a simulation, physical dissipation of the velocity field and species mass fraction often compete with numerical dissipation arising from the errors of the numerical method. In a DNS the computational mesh resolves all physical gradients of the flow and the relative effect of numerical dissipation is small. In LES, unresolved scales are present and numerical dissipation can have a large impact on the flow, depending on the computational mesh. A suite of simulations explores the space between these two extremes by studying the effects of grid resolution, Reynolds number and numerical method on the mixing process. Results from a DNS are shown using two different codes, which use a high- and low-order numerical method and show convergence in the temporal and spectral dependent quantities associated with mixing. Data from a coarse LES are also presented and include a grid convergence study. A model for an effective viscosity is proposed which allows for an a posteriori analysis of the simulation data that is agnostic to the LES model, numerics and the physical Reynolds number of the simulation. An analogous approximation for an effective species diffusivity is also presented. This framework can then be used to estimate the effective Reynolds number and Schmidt number of future simulations, elucidate the impact of numerical dissipation on the mixing process for an arbitrary numerical method and provide guidance for resolution requirements of future calculations},
	number = {4},
	urldate = {2020-01-14},
	journal = {Physics of Fluids},
	author = {Olson, Britton J. and Greenough, Jeffrey A.},
	month = apr,
	year = {2014},
	note = {arXiv: 1402.2665},
	keywords = {Physics - Fluid Dynamics},
	pages = {044103},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/ZG69HRFP/Olson and Greenough - 2014 - Large eddy simulation requirements for the Richtmy.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/B7PG6DMM/1402.html:text/html}
}

@article{wang_generating_2020,
	title = {Generating {Semantic} {Adversarial} {Examples} via {Feature} {Manipulation}},
	url = {http://arxiv.org/abs/2001.02297},
	abstract = {The vulnerability of deep neural networks to adversarial attacks has been widely demonstrated (e.g., adversarial example attacks). Traditional attacks perform unstructured pixel-wise perturbation to fool the classifier. An alternative approach is to have perturbations in the latent space. However, such perturbations are hard to control due to the lack of interpretability and disentanglement. In this paper, we propose a more practical adversarial attack by designing structured perturbation with semantic meanings. Our proposed technique manipulates the semantic attributes of images via the disentangled latent codes. The intuition behind our technique is that images in similar domains have some commonly shared but theme-independent semantic attributes, e.g. thickness of lines in handwritten digits, that can be bidirectionally mapped to disentangled latent codes. We generate adversarial perturbation by manipulating a single or a combination of these latent codes and propose two unsupervised semantic manipulation approaches: vector-based disentangled representation and feature map-based disentangled representation, in terms of the complexity of the latent codes and smoothness of the reconstructed images. We conduct extensive experimental evaluations on real-world image data to demonstrate the power of our attacks for black-box classifiers. We further demonstrate the existence of a universal, image-agnostic semantic adversarial example.},
	urldate = {2020-01-14},
	journal = {arXiv:2001.02297 [cs, stat]},
	author = {Wang, Shuo and Chen, Shangyu and Chen, Tianle and Nepal, Surya and Rudolph, Carsten and Grobler, Marthie},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.02297},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: arXiv admin note: substantial text overlap with arXiv:1705.09064 by other authors},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/QX4XCVE3/Wang et al. - 2020 - Generating Semantic Adversarial Examples via Featu.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/TX7IRIGX/2001.html:text/html}
}

@article{mohapatra_towards_2019,
	title = {Towards {Verifying} {Robustness} of {Neural} {Networks} {Against} {Semantic} {Perturbations}},
	url = {http://arxiv.org/abs/1912.09533},
	abstract = {Verifying robustness of neural networks given a specified threat model is a fundamental yet challenging task. While current verification methods mainly focus on the L\_p-norm-ball threat model of the input instances, robustness verification against semantic adversarial attacks inducing large L\_p-norm perturbations such as color shifting and lighting adjustment are beyond their capacity. To bridge this gap, we propose Semantify-NN, a model-agnostic and generic robustness verification approach against semantic perturbations for neural networks. By simply inserting our proposed semantic perturbation layers (SP-layers) to the input layer of any given model, Semantify-NN is model-agnostic, and any \$L\_p\$-norm-ball based verification tools can be used to verify the model robustness against semantic perturbations. We illustrate the principles of designing the SP-layers and provide examples including semantic perturbations to image classification in the space of hue, saturation, lightness, brightness, contrast and rotation, respectively. Experimental results on various network architectures and different datasets demonstrate the superior verification performance of Semantify-NN over L\_p-norm-based verification frameworks that naively convert semantic perturbation to L\_p-norm. To the best of our knowledge, Semantify-NN is the first framework to support robustness verification against a wide range of semantic perturbations.},
	urldate = {2020-01-14},
	journal = {arXiv:1912.09533 [cs, stat]},
	author = {Mohapatra, Jeet and Tsui-Wei and Weng and Chen, Pin-Yu and Liu, Sijia and Daniel, Luca},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.09533},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/AAPDLNBB/Mohapatra et al. - 2019 - Towards Verifying Robustness of Neural Networks Ag.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/RVKYC3BE/1912.html:text/html}
}

@article{saha_attack_2019,
	title = {Attack {Agnostic} {Statistical} {Method} for {Adversarial} {Detection}},
	url = {http://arxiv.org/abs/1911.10008},
	abstract = {Deep Learning based AI systems have shown great promise in various domains such as vision, audio, autonomous systems (vehicles, drones), etc. Recent research on neural networks has shown the susceptibility of deep networks to adversarial attacks - a technique of adding small perturbations to the inputs which can fool a deep network into misclassifying them. Developing defenses against such adversarial attacks is an active research area, with some approaches proposing robust models that are immune to such adversaries, while other techniques attempt to detect such adversarial inputs. In this paper, we present a novel statistical approach for adversarial detection in image classification. Our approach is based on constructing a per-class feature distribution and detecting adversaries based on comparison of features of a test image with the feature distribution of its class. For this purpose, we make use of various statistical distances such as ED (Energy Distance), MMD (Maximum Mean Discrepancy) for adversarial detection, and analyze the performance of each metric. We experimentally show that our approach achieves good adversarial detection performance on MNIST and CIFAR-10 datasets irrespective of the attack method, sample size and the degree of adversarial perturbation.},
	urldate = {2020-01-14},
	journal = {arXiv:1911.10008 [cs, stat]},
	author = {Saha, Sambuddha and Kumar, Aashish and Sahay, Pratyush and Jose, George and Kruthiventi, Srinivas and Muralidhara, Harikrishna},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.10008},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/5GG2NKRM/Saha et al. - 2019 - Attack Agnostic Statistical Method for Adversarial.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/W6CZYYZ8/1911.html:text/html}
}

@article{gao_universal_2019,
	title = {Universal {Adversarial} {Perturbation} for {Text} {Classification}},
	url = {http://arxiv.org/abs/1910.04618},
	abstract = {Given a state-of-the-art deep neural network text classifier, we show the existence of a universal and very small perturbation vector (in the embedding space) that causes natural text to be misclassified with high probability. Unlike images on which a single fixed-size adversarial perturbation can be found, text is of variable length, so we define the "universality" as "token-agnostic", where a single perturbation is applied to each token, resulting in different perturbations of flexible sizes at the sequence level. We propose an algorithm to compute universal adversarial perturbations, and show that the state-of-the-art deep neural networks are highly vulnerable to them, even though they keep the neighborhood of tokens mostly preserved. We also show how to use these adversarial perturbations to generate adversarial text samples. The surprising existence of universal "token-agnostic" adversarial perturbations may reveal important properties of a text classifier.},
	urldate = {2020-01-14},
	journal = {arXiv:1910.04618 [cs, stat]},
	author = {Gao, Hang and Oates, Tim},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.04618},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/K8R4HFCC/Gao and Oates - 2019 - Universal Adversarial Perturbation for Text Classi.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/VFBSRHUD/1910.html:text/html}
}

@article{co_sensitivity_2019,
	title = {Sensitivity of {Deep} {Convolutional} {Networks} to {Gabor} {Noise}},
	url = {http://arxiv.org/abs/1906.03455},
	abstract = {Deep Convolutional Networks (DCNs) have been shown to be sensitive to Universal Adversarial Perturbations (UAPs): input-agnostic perturbations that fool a model on large portions of a dataset. These UAPs exhibit interesting visual patterns, but this phenomena is, as yet, poorly understood. Our work shows that visually similar procedural noise patterns also act as UAPs. In particular, we demonstrate that different DCN architectures are sensitive to Gabor noise patterns. This behaviour, its causes, and implications deserve further in-depth study.},
	urldate = {2020-01-14},
	journal = {arXiv:1906.03455 [cs, stat]},
	author = {Co, Kenneth T. and Muñoz-González, Luis and Lupu, Emil C.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.03455},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: Accepted to ICML 2019 Workshop on Identifying and Understanding Deep Learning Phenomena},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/YWFGIA5R/Co et al. - 2019 - Sensitivity of Deep Convolutional Networks to Gabo.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/9VP7RFUP/1906.html:text/html}
}

@article{borkar_defending_2019,
	title = {Defending {Against} {Universal} {Attacks} {Through} {Selective} {Feature} {Regeneration}},
	url = {http://arxiv.org/abs/1906.03444},
	abstract = {Deep neural network (DNN) predictions have been shown to be vulnerable to carefully crafted adversarial perturbations. Specifically, image-agnostic (universal adversarial) perturbations added to any image can fool a target network into making erroneous predictions. Departing from existing defense strategies that work mostly in the image domain, we present a novel defense which operates in the DNN feature domain and effectively defends against such universal perturbations. Our approach identifies pre-trained convolutional features that are most vulnerable to adversarial noise and deploys trainable feature regeneration units which transform these DNN filter activations into resilient features that are robust to universal perturbations. Regenerating only the top 50\% adversarially susceptible activations in at most 6 DNN layers and leaving all remaining DNN activations unchanged, we outperform existing defense strategies across different network architectures by more than 10\% in restored accuracy. We show that without any additional modification, our defense trained on ImageNet with one type of universal attack examples effectively defends against other types of unseen universal attacks.},
	urldate = {2020-01-14},
	journal = {arXiv:1906.03444 [cs]},
	author = {Borkar, Tejas and Heide, Felix and Karam, Lina},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.03444},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/HDG8FGE6/Borkar et al. - 2019 - Defending Against Universal Attacks Through Select.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/JPYFZREU/1906.html:text/html}
}

@article{david_ganchors_2019,
	title = {{GANchors}: {Realistic} {Image} {Perturbation} {Distributions} for {Anchors} {Using} {Generative} {Models}},
	shorttitle = {{GANchors}},
	url = {http://arxiv.org/abs/1906.00297},
	abstract = {We extend and improve the work of Model Agnostic Anchors for explanations on image classification through the use of generative adversarial networks (GANs). Using GANs, we generate samples from a more realistic perturbation distribution, by optimizing under a lower dimensional latent space. This increases the trust in an explanation, as results now come from images that are more likely to be found in the original training set of a classifier, rather than an overlay of random images. A large drawback to our method is the computational complexity of sampling through optimization; to address this, we implement more efficient algorithms, including a diverse encoder. Lastly, we share results from the MNIST and CelebA datasets, and note that our explanations can lead to smaller and higher precision anchors.},
	urldate = {2020-01-14},
	journal = {arXiv:1906.00297 [cs, stat]},
	author = {David, Kurtis Evan and Keane, Harrison and Noh, Jun Min},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.00297},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Final project for the Fair and Transparent Machine Learning course at UT Austin -- taught by Dr. Joydeep Ghosh},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/ZFUR4NWN/David et al. - 2019 - GANchors Realistic Image Perturbation Distributio.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/AUUEJE5X/1906.html:text/html}
}

@article{singla_robustness_2019,
	title = {Robustness {Certificates} {Against} {Adversarial} {Examples} for {ReLU} {Networks}},
	url = {http://arxiv.org/abs/1902.01235},
	abstract = {While neural networks have achieved high performance in different learning tasks, their accuracy drops significantly in the presence of small adversarial perturbations to inputs. Defenses based on regularization and adversarial training are often followed by new attacks to defeat them. In this paper, we propose attack-agnostic robustness certificates for a multi-label classification problem using a deep ReLU network. Although computing the exact distance of a given input sample to the classification decision boundary requires solving a non-convex optimization, we characterize two lower bounds for such distances, namely the simplex certificate and the decision boundary certificate. These robustness certificates leverage the piece-wise linear structure of ReLU networks and use the fact that in a polyhedron around a given sample, the prediction function is linear. In particular, the proposed simplex certificate has a closed-form, is differentiable and is an order of magnitude faster to compute than the existing methods even for deep networks. In addition to theoretical bounds, we provide numerical results for our certificates over MNIST and compare them with some existing upper bounds.},
	urldate = {2020-01-14},
	journal = {arXiv:1902.01235 [cs, stat]},
	author = {Singla, Sahil and Feizi, Soheil},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.01235},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/4Y7YN24Z/Singla and Feizi - 2019 - Robustness Certificates Against Adversarial Exampl.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/KH7TXK3N/1902.html:text/html}
}

@article{azarafrooz_fuzzy_2018,
	title = {Fuzzy {Hashing} as {Perturbation}-{Consistent} {Adversarial} {Kernel} {Embedding}},
	url = {http://arxiv.org/abs/1812.07071},
	abstract = {Measuring the similarity of two files is an important task in malware analysis, with fuzzy hash functions being a popular approach. Traditional fuzzy hash functions are data agnostic: they do not learn from a particular dataset how to determine similarity; their behavior is fixed across all datasets. In this paper, we demonstrate that fuzzy hash functions can be learned in a novel minimax training framework and that these learned fuzzy hash functions outperform traditional fuzzy hash functions at the file similarity task for Portable Executable files. In our approach, hash digests can be extracted from the kernel embeddings of two kernel networks, trained in a minimax framework, where the roles of players during training (i.e adversary versus generator) alternate along with the input data. We refer to this new minimax architecture as perturbation-consistent. The similarity score for a pair of files is the utility of the minimax game in equilibrium. Our experiments show that learned fuzzy hash functions generalize well, capable of determining that two files are similar even when one of those files was generated using insertion and deletion operations.},
	urldate = {2020-01-14},
	journal = {arXiv:1812.07071 [cs, stat]},
	author = {Azarafrooz, Ari and Brock, John},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.07071},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/KRIJ5YAB/Azarafrooz and Brock - 2018 - Fuzzy Hashing as Perturbation-Consistent Adversari.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/WEKALR7R/1812.html:text/html}
}

@article{li_universal_2019-1,
	title = {Universal {Perturbation} {Attack} {Against} {Image} {Retrieval}},
	url = {http://arxiv.org/abs/1812.00552},
	abstract = {Universal adversarial perturbations (UAPs), a.k.a. input-agnostic perturbations, has been proved to exist and be able to fool cutting-edge deep learning models on most of the data samples. Existing UAP methods mainly focus on attacking image classification models. Nevertheless, little attention has been paid to attacking image retrieval systems. In this paper, we make the first attempt in attacking image retrieval systems. Concretely, image retrieval attack is to make the retrieval system return irrelevant images to the query at the top ranking list. It plays an important role to corrupt the neighbourhood relationships among features in image retrieval attack. To this end, we propose a novel method to generate retrieval-against UAP to break the neighbourhood relationships of image features via degrading the corresponding ranking metric. To expand the attack method to scenarios with varying input sizes or untouchable network parameters, a multi-scale random resizing scheme and a ranking distillation strategy are proposed. We evaluate the proposed method on four widely-used image retrieval datasets, and report a significant performance drop in terms of different metrics, such as mAP and mP@10. Finally, we test our attack methods on the real-world visual search engine, i.e., Google Images, which demonstrates the practical potentials of our methods.},
	urldate = {2020-01-14},
	journal = {arXiv:1812.00552 [cs]},
	author = {Li, Jie and Ji, Rongrong and Liu, Hong and Hong, Xiaopeng and Gao, Yue and Tian, Qi},
	month = sep,
	year = {2019},
	note = {arXiv: 1812.00552},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	annote = {Comment: ICCV 2019},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/I5NK4867/Li et al. - 2019 - Universal Perturbation Attack Against Image Retrie.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/PYLRIL9R/1812.html:text/html}
}

@article{sun_adversarial_2019,
	title = {Adversarial {Defense} by {Stratified} {Convolutional} {Sparse} {Coding}},
	url = {http://arxiv.org/abs/1812.00037},
	abstract = {We propose an adversarial defense method that achieves state-of-the-art performance among attack-agnostic adversarial defense methods while also maintaining robustness to input resolution, scale of adversarial perturbation, and scale of dataset size. Based on convolutional sparse coding, we construct a stratified low-dimensional quasi-natural image space that faithfully approximates the natural image space while also removing adversarial perturbations. We introduce a novel Sparse Transformation Layer (STL) in between the input image and the first layer of the neural network to efficiently project images into our quasi-natural image space. Our experiments show state-of-the-art performance of our method compared to other attack-agnostic adversarial defense methods in various adversarial settings.},
	urldate = {2020-01-14},
	journal = {arXiv:1812.00037 [cs]},
	author = {Sun, Bo and Tsai, Nian-hsuan and Liu, Fangchen and Yu, Ronald and Su, Hao},
	month = jun,
	year = {2019},
	note = {arXiv: 1812.00037},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published at CVPR 2019},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/QNNVKW3Q/Sun et al. - 2019 - Adversarial Defense by Stratified Convolutional Sp.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/BFF99PH2/1812.html:text/html}
}

@article{raghunathan_semidefinite_2018,
	title = {Semidefinite relaxations for certifying robustness to adversarial examples},
	url = {http://arxiv.org/abs/1811.01057},
	abstract = {Despite their impressive performance on diverse tasks, neural networks fail catastrophically in the presence of adversarial inputs---imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses, ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper, we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different "foreign networks" whose training objectives are agnostic to our proposed relaxation.},
	urldate = {2020-01-14},
	journal = {arXiv:1811.01057 [cs, stat]},
	author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.01057},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: To appear at NIPS 2018},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/B4HRLGN6/Raghunathan et al. - 2018 - Semidefinite relaxations for certifying robustness.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/EEMNSAF3/1811.html:text/html}
}

@article{co_procedural_2019,
	title = {Procedural {Noise} {Adversarial} {Examples} for {Black}-{Box} {Attacks} on {Deep} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1810.00470},
	doi = {10.1145/3319535.3345660},
	abstract = {Deep Convolutional Networks (DCNs) have been shown to be vulnerable to adversarial examples---perturbed inputs specifically designed to produce intentional errors in the learning algorithms at test time. Existing input-agnostic adversarial perturbations exhibit interesting visual patterns that are currently unexplained. In this paper, we introduce a structured approach for generating Universal Adversarial Perturbations (UAPs) with procedural noise functions. Our approach unveils the systemic vulnerability of popular DCN models like Inception v3 and YOLO v3, with single noise patterns able to fool a model on up to 90\% of the dataset. Procedural noise allows us to generate a distribution of UAPs with high universal evasion rates using only a few parameters. Additionally, we propose Bayesian optimization to efficiently learn procedural noise parameters to construct inexpensive untargeted black-box attacks. We demonstrate that it can achieve an average of less than 10 queries per successful attack, a 100-fold improvement on existing methods. We further motivate the use of input-agnostic defences to increase the stability of models to adversarial perturbations. The universality of our attacks suggests that DCN models may be sensitive to aggregations of low-level class-agnostic features. These findings give insight on the nature of some universal adversarial perturbations and how they could be generated in other applications.},
	urldate = {2020-01-14},
	journal = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security  - CCS '19},
	author = {Co, Kenneth T. and Muñoz-González, Luis and de Maupeou, Sixte and Lupu, Emil C.},
	year = {2019},
	note = {arXiv: 1810.00470},
	keywords = {Statistics - Machine Learning, Computer Science - Cryptography and Security},
	pages = {275--289},
	annote = {Comment: 16 pages, 10 figures. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19)},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/F967TQJV/Co et al. - 2019 - Procedural Noise Adversarial Examples for Black-Bo.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/M2ZML457/1810.html:text/html}
}

@article{tsuzuku_structural_2019,
	title = {On the {Structural} {Sensitivity} of {Deep} {Convolutional} {Networks} to the {Directions} of {Fourier} {Basis} {Functions}},
	url = {http://arxiv.org/abs/1809.04098},
	abstract = {Data-agnostic quasi-imperceptible perturbations on inputs are known to degrade recognition accuracy of deep convolutional networks severely. This phenomenon is considered to be a potential security issue. Moreover, some results on statistical generalization guarantees indicate that the phenomenon can be a key to improve the networks' generalization. However, the characteristics of the shared directions of such harmful perturbations remain unknown. Our primal finding is that convolutional networks are sensitive to the directions of Fourier basis functions. We derived the property by specializing a hypothesis of the cause of the sensitivity, known as the linearity of neural networks, to convolutional networks and empirically validated it. As a by-product of the analysis, we propose an algorithm to create shift-invariant universal adversarial perturbations available in black-box settings.},
	urldate = {2020-01-14},
	journal = {arXiv:1809.04098 [cs]},
	author = {Tsuzuku, Yusuke and Sato, Issei},
	month = apr,
	year = {2019},
	note = {arXiv: 1809.04098},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2019},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/4ZF8WKQD/Tsuzuku and Sato - 2019 - On the Structural Sensitivity of Deep Convolutiona.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/JU4Z6BDB/1809.html:text/html}
}

@article{mopuri_nag_2018,
	title = {{NAG}: {Network} for {Adversary} {Generation}},
	shorttitle = {{NAG}},
	url = {http://arxiv.org/abs/1712.03390},
	abstract = {Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations. However, for a given classifier, they generate one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. Also, in order to build robust models, it is essential to explore the manifold of adversarial perturbations. In this paper, we propose for the first time, a generative approach to model the distribution of adversarial perturbations. The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. Our trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. Our experimental evaluation demonstrates that perturbations crafted by our model (i) achieve state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver excellent cross model generalizability. Our work can be deemed as an important step in the process of inferring about the complex manifolds of adversarial perturbations.},
	urldate = {2020-01-14},
	journal = {arXiv:1712.03390 [cs]},
	author = {Mopuri, Konda Reddy and Ojha, Utkarsh and Garg, Utsav and Babu, R. Venkatesh},
	month = mar,
	year = {2018},
	note = {arXiv: 1712.03390},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2018},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/N7355MUD/Mopuri et al. - 2018 - NAG Network for Adversary Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/QT3WJC4V/1712.html:text/html}
}

@article{poursaeed_generative_2018,
	title = {Generative {Adversarial} {Perturbations}},
	url = {http://arxiv.org/abs/1712.02328},
	abstract = {In this paper, we propose novel generative models for creating adversarial examples, slightly perturbed images resembling natural images but maliciously crafted to fool pre-trained models. We present trainable deep neural networks for transforming images to adversarial perturbations. Our proposed models can produce image-agnostic and image-dependent perturbations for both targeted and non-targeted attacks. We also demonstrate that similar architectures can achieve impressive results in fooling classification and semantic segmentation models, obviating the need for hand-crafting attack methods for each task. Using extensive experiments on challenging high-resolution datasets such as ImageNet and Cityscapes, we show that our perturbations achieve high fooling rates with small perturbation norms. Moreover, our attacks are considerably faster than current iterative methods at inference time.},
	urldate = {2020-01-14},
	journal = {arXiv:1712.02328 [cs, stat]},
	author = {Poursaeed, Omid and Katsman, Isay and Gao, Bicheng and Belongie, Serge},
	month = jul,
	year = {2018},
	note = {arXiv: 1712.02328},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Cryptography and Security},
	annote = {Comment: CVPR 2018, camera-ready version},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/YMEE5SMM/Poursaeed et al. - 2018 - Generative Adversarial Perturbations.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/TWJ6AX24/1712.html:text/html}
}

@article{khrulkov_art_2017,
	title = {Art of singular vectors and universal adversarial perturbations},
	url = {http://arxiv.org/abs/1709.03582},
	abstract = {Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been attracting a lot of attention in recent studies. It has been shown that for many state of the art DNNs performing image classification there exist universal adversarial perturbations --- image-agnostic perturbations mere addition of which to natural images with high probability leads to their misclassification. In this work we propose a new algorithm for constructing such universal perturbations. Our approach is based on computing the so-called \$(p, q)\$-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns, and by using only 64 images we were able to construct universal perturbations with more than 60 {\textbackslash}\% fooling rate on the dataset consisting of 50000 images. We also investigate a correlation between the maximal singular value of the Jacobian matrix and the fooling rate of the corresponding singular vector, and show that the constructed perturbations generalize across networks.},
	urldate = {2020-01-14},
	journal = {arXiv:1709.03582 [cs]},
	author = {Khrulkov, Valentin and Oseledets, Ivan},
	month = nov,
	year = {2017},
	note = {arXiv: 1709.03582},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to CVPR 2018},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/4V4JIHUE/Khrulkov and Oseledets - 2017 - Art of singular vectors and universal adversarial .pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/WAC9DET6/1709.html:text/html}
}

@article{mopuri_fast_2017,
	title = {Fast {Feature} {Fool}: {A} data independent approach to universal adversarial perturbations},
	shorttitle = {Fast {Feature} {Fool}},
	url = {http://arxiv.org/abs/1707.05572},
	abstract = {State-of-the-art object recognition Convolutional Neural Networks (CNNs) are shown to be fooled by image agnostic perturbations, called universal adversarial perturbations. It is also observed that these perturbations generalize across multiple networks trained on the same target data. However, these algorithms require training data on which the CNNs were trained and compute adversarial perturbations via complex optimization. The fooling performance of these approaches is directly proportional to the amount of available training data. This makes them unsuitable for practical attacks since its unreasonable for an attacker to have access to the training data. In this paper, for the first time, we propose a novel data independent approach to generate image agnostic perturbations for a range of CNNs trained for object recognition. We further show that these perturbations are transferable across multiple network architectures trained either on same or different data. In the absence of data, our method generates universal adversarial perturbations efficiently via fooling the features learned at multiple layers thereby causing CNNs to misclassify. Experiments demonstrate impressive fooling rates and surprising transferability for the proposed universal perturbations generated without any training data.},
	urldate = {2020-01-14},
	journal = {arXiv:1707.05572 [cs]},
	author = {Mopuri, Konda Reddy and Garg, Utsav and Babu, R. Venkatesh},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.05572},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: BMVC 2017 and codes are available at https://github.com/utsavgarg/fast-feature-fool},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/W6BZQ7FI/Mopuri et al. - 2017 - Fast Feature Fool A data independent approach to .pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/YTN9VSS2/1707.html:text/html}
}

@article{moosavi-dezfooli_analysis_2017,
	title = {Analysis of universal adversarial perturbations},
	url = {http://arxiv.org/abs/1705.09554},
	abstract = {Deep networks have recently been shown to be vulnerable to universal perturbations: there exist very small image-agnostic perturbations that cause most natural images to be misclassified by such classifiers. In this paper, we propose the first quantitative analysis of the robustness of classifiers to universal perturbations, and draw a formal link between the robustness to universal perturbations, and the geometry of the decision boundary. Specifically, we establish theoretical bounds on the robustness of classifiers under two decision boundary models (flat and curved models). We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of their curvature: there exists shared directions along which the decision boundary of deep networks is systematically positively curved. Under such conditions, we prove the existence of small universal perturbations. Our analysis further provides a novel geometric method for computing universal perturbations, in addition to explaining their properties.},
	urldate = {2020-01-14},
	journal = {arXiv:1705.09554 [cs, stat]},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal and Soatto, Stefano},
	month = may,
	year = {2017},
	note = {arXiv: 1705.09554},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/45CDNL3J/Moosavi-Dezfooli et al. - 2017 - Analysis of universal adversarial perturbations.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/SVSETC2U/1705.html:text/html}
}

@article{metzen_universal_2017,
	title = {Universal {Adversarial} {Perturbations} {Against} {Semantic} {Image} {Segmentation}},
	url = {http://arxiv.org/abs/1704.05712},
	abstract = {While deep learning is remarkably successful on perceptual tasks, it was also shown to be vulnerable to adversarial perturbations of the input. These perturbations denote noise added to the input that was generated specifically to fool the system while being quasi-imperceptible for humans. More severely, there even exist universal perturbations that are input-agnostic but fool the network on the majority of inputs. While recent work has focused on image classification, this work proposes attacks against semantic image segmentation: we present an approach for generating (universal) adversarial perturbations that make the network yield a desired target segmentation as output. We show empirically that there exist barely perceptible universal noise patterns which result in nearly the same predicted segmentation for arbitrary inputs. Furthermore, we also show the existence of universal noise which removes a target class (e.g., all pedestrians) from the segmentation while leaving the segmentation mostly unchanged otherwise.},
	urldate = {2020-01-14},
	journal = {arXiv:1704.05712 [cs, stat]},
	author = {Metzen, Jan Hendrik and Kumar, Mummadi Chaithanya and Brox, Thomas and Fischer, Volker},
	month = jul,
	year = {2017},
	note = {arXiv: 1704.05712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Final version for ICCV including supplementary material},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/CBIKKR9X/Metzen et al. - 2017 - Universal Adversarial Perturbations Against Semant.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/5K2RFNCM/1704.html:text/html}
}

@article{wang_netrex_2017,
	title = {{NetREX}: {Network} {Rewiring} using {EXpression} - {Towards} {Context} {Specific} {Regulatory} {Networks}},
	shorttitle = {{NetREX}},
	url = {http://arxiv.org/abs/1704.05343},
	abstract = {Understanding gene regulation is a fundamental step towards understanding of how cells function and respond to environmental cues and perturbations. An important step in this direction is to infer the transcription factor-gene regulatory network (GRN). However gene regulatory networks are typically constructed disregarding the fact that regulatory programs are conditioned on tissue type, developmental stage, sex, and other factors. Collecting multitude of features required for a reliable construction of GRNs such as physical features and functional features for every context of interest is costly. Therefore we need methods that is able to use the knowledge of a context-agnostic network for construction of a context specific regulatory network. To address this challenge we developed a computational approach that uses context specific expression data and a GRN constructed in a different but related context to construct a context specific GRN. Our method, NetREX, is inspired by network component analysis that estimates TF activities and their influences on target genes given predetermined topology of a TF-gene network. To predict a network under a different condition, NetREX removes the restriction that the topology of the TF-gene network is fixed and allows for adding and removing edges to that network. To solve the corresponding optimization problem, we provide a general mathematical framework allowing use of the recently proposed PALM technique and develop a convergent algorithm. We tested our NetREX on simulated data and subsequently applied it to gene expression data in adult females from Drosophila deletion (DrosDel) panel. The networks predicted by NetREX showed higher biological consistency than alternative approaches. In addition, we used the list of recently identified targets of the Doublesex (DSX) transcription factor to demonstrate the predictive power of our method.},
	urldate = {2020-01-14},
	journal = {arXiv:1704.05343 [q-bio]},
	author = {Wang, Yijie and Cho, Dong-Yeon and Lee, Hangnoh and Fear, Justin and Oliver, Brian and Przytycka, Teresa M.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.05343},
	keywords = {Quantitative Biology - Molecular Networks},
	annote = {Comment: RECOMB 2017},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/PB935IZ5/Wang et al. - 2017 - NetREX Network Rewiring using EXpression - Toward.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/BR2R3IBA/1704.html:text/html}
}

@article{fong_interpretable_2017,
	title = {Interpretable {Explanations} of {Black} {Boxes} by {Meaningful} {Perturbation}},
	url = {http://arxiv.org/abs/1704.03296},
	doi = {10.1109/ICCV.2017.371},
	abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.},
	urldate = {2020-01-14},
	journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
	author = {Fong, Ruth and Vedaldi, Andrea},
	month = oct,
	year = {2017},
	note = {arXiv: 1704.03296},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	pages = {3449--3457},
	annote = {Comment: Final camera-ready paper published at ICCV 2017 (Supplementary materials: http://openaccess.thecvf.com/content\_ICCV\_2017/supplemental/Fong\_Interpretable\_Explanations\_of\_ICCV\_2017\_supplemental.pdf)},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/EV9K6XDS/Fong and Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meani.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/FEPBVSE7/1704.html:text/html}
}

@article{DeepFool-Moosavi-Dezfooli15,
  author    = {Seyed{-}Mohsen Moosavi{-}Dezfooli and
               Alhussein Fawzi and
               Pascal Frossard},
  title     = {DeepFool: a simple and accurate method to fool deep neural networks},
  journal   = {CoRR},
  volume    = {abs/1511.04599},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.04599},
  archivePrefix = {arXiv},
  eprint    = {1511.04599},
  timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Moosavi-Dezfooli15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{moosavi-dezfooli_universal_2017,
	title = {Universal adversarial perturbations},
	url = {http://arxiv.org/abs/1610.08401},
	abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.},
	urldate = {2020-01-14},
	journal = {arXiv:1610.08401 [cs, stat]},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	month = mar,
	year = {2017},
	note = {arXiv: 1610.08401},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/82REV69N/Moosavi-Dezfooli et al. - 2017 - Universal adversarial perturbations.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/PDUCMV9X/1610.html:text/html}
}

@article{raff2020quantifying,
author = {Raff, Edward},
title = {Quantifying Independently Reproducible Machine Learning},
journal = {The Gradient},
year = {2020},
howpublished = {\url{https://thegradient.pub/independently-reproducible-machine-learning/ } },
}
@article{karianakis_boosting_2015,
	title = {Boosting {Convolutional} {Features} for {Robust} {Object} {Proposals}},
	url = {http://arxiv.org/abs/1503.06350},
	abstract = {Deep Convolutional Neural Networks (CNNs) have demonstrated excellent performance in image classification, but still show room for improvement in object-detection tasks with many categories, in particular for cluttered scenes and occlusion. Modern detection algorithms like Regions with CNNs (Girshick et al., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions which with high probability represent objects, where in turn CNNs are deployed for classification. Selective Search represents a family of sophisticated algorithms that are engineered with multiple segmentation, appearance and saliency cues, typically coming with a significant run-time overhead. Furthermore, (Hosang et al., 2014) have shown that most methods suffer from low reproducibility due to unstable superpixels, even for slight image perturbations. Although CNNs are subsequently used for classification in top-performing object-detection pipelines, current proposal methods are agnostic to how these models parse objects and their rich learned representations. As a result they may propose regions which may not resemble high-level objects or totally miss some of them. To overcome these drawbacks we propose a boosting approach which directly takes advantage of hierarchical CNN features for detecting regions of interest fast. We demonstrate its performance on ImageNet 2013 detection benchmark and compare it with state-of-the-art methods.},
	urldate = {2020-01-14},
	journal = {arXiv:1503.06350 [cs]},
	author = {Karianakis, Nikolaos and Fuchs, Thomas J. and Soatto, Stefano},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.06350},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 9 pages, 4 figures, 2 tables, 42 references},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/KSP2WK3V/Karianakis et al. - 2015 - Boosting Convolutional Features for Robust Object .pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/4FJPKNKX/1503.html:text/html}
}

@misc{noauthor_pdf_nodate,
	title = {[{PDF}] {Transferability} of {Adversarial} {Attacks} in {Model}-{Agnostic} {Meta}-{Learning} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Transferability-of-Adversarial-Attacks-in-Edmunds-Kuznetsov/ebc3abb53b9d1f32cbc36fcce7e0d4384f103715},
	urldate = {2020-01-16},
	file = {[PDF] Transferability of Adversarial Attacks in Model-Agnostic Meta-Learning | Semantic Scholar:/Users/maumau/Zotero/storage/94E3ADX5/ebc3abb53b9d1f32cbc36fcce7e0d4384f103715.html:text/html}
}

@misc{noauthor_model_nodate,
	title = {Model {Agnostic} {Dual} {Quality} {Assessment} for {Adversarial} {Machine} {Learning} and an {Analysis} of {Current} {Neural} {Networks} and {Defenses} {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Model-Agnostic-Dual-Quality-Assessment-for-Machine-Vargas-Kotyan/2448463d3b477528657d481c60a00dedb783b7f4},
	urldate = {2020-01-16},
	file = {Model Agnostic Dual Quality Assessment for Adversarial Machine Learning and an Analysis of Current Neural Networks and Defenses | Semantic Scholar:/Users/maumau/Zotero/storage/F8CTWK96/2448463d3b477528657d481c60a00dedb783b7f4.html:text/html}
}

@misc{noauthor_are_nodate,
	title = {Are {Image}-{Agnostic} {Universal} {Adversarial} {Perturbations} for {Face} {Recognition} {Difficult} to {Detect}? {\textbar} {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Are-Image-Agnostic-Universal-Adversarial-for-Face-Agarwal-Singh/2bf8827eb761ea755909065a06af18b86d363001},
	urldate = {2020-01-16},
	file = {Are Image-Agnostic Universal Adversarial Perturbations for Face Recognition Difficult to Detect? | Semantic Scholar:/Users/maumau/Zotero/storage/R2EW6AJX/2bf8827eb761ea755909065a06af18b86d363001.html:text/html}
}

@misc{noauthor_generalizable_nodate,
	title = {Generalizable {Data}-{Free} {Objective} for {Crafting} {Universal} {Adversarial} {Perturbations} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8423654},
	urldate = {2020-01-16},
	file = {Generalizable Data-Free Objective for Crafting Universal Adversarial Perturbations - IEEE Journals & Magazine:/Users/maumau/Zotero/storage/LS5SZIYS/8423654.html:text/html}
}

@misc{noauthor_multi-scale_nodate,
	title = {Multi-{Scale} {Interpretation} {Model} for {Convolutional} {Neural} {Networks}: {Building} {Trust} {Based} on {Hierarchical} {Interpretation} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8653995},
	urldate = {2020-01-16},
	file = {Multi-Scale Interpretation Model for Convolutional Neural Networks\: Building Trust Based on Hierarchical Interpretation - IEEE Journals & Magazine:/Users/maumau/Zotero/storage/LNXJZX8F/8653995.html:text/html}
}

@misc{noauthor_are_nodate-1,
	title = {Are {Image}-{Agnostic} {Universal} {Adversarial} {Perturbations} for {Face} {Recognition} {Difficult} to {Detect}? - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8698548},
	urldate = {2020-01-16},
	file = {Are Image-Agnostic Universal Adversarial Perturbations for Face Recognition Difficult to Detect? - IEEE Conference Publication:/Users/maumau/Zotero/storage/6WWS72E3/8698548.html:text/html}
}

@misc{noauthor_are_nodate-2,
	title = {Are {Image}-{Agnostic} {Universal} {Adversarial} {Perturbations} for {Face} {Recognition} {Difficult} to {Detect}? - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8698548},
	urldate = {2020-01-16},
	file = {Are Image-Agnostic Universal Adversarial Perturbations for Face Recognition Difficult to Detect? - IEEE Conference Publication:/Users/maumau/Zotero/storage/Y6GQ94W8/8698548.html:text/html}
}

@misc{noauthor_defense_nodate,
	title = {Defense {Against} {Universal} {Adversarial} {Perturbations} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8578455},
	urldate = {2020-01-16},
	file = {Defense Against Universal Adversarial Perturbations - IEEE Conference Publication:/Users/maumau/Zotero/storage/GQ9KQIY6/8578455.html:text/html}
}

@misc{noauthor_generative_nodate,
	title = {Generative {Adversarial} {Perturbations} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8578563},
	urldate = {2020-01-16},
	file = {Generative Adversarial Perturbations - IEEE Conference Publication:/Users/maumau/Zotero/storage/TCT72WFZ/8578563.html:text/html}
}

@misc{noauthor_nag_nodate,
	title = {{NAG}: {Network} for {Adversary} {Generation} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8578182},
	urldate = {2020-01-16},
	file = {NAG\: Network for Adversary Generation - IEEE Conference Publication:/Users/maumau/Zotero/storage/YVAHTX3D/8578182.html:text/html}
}

@misc{noauthor_nag_nodate-1,
	title = {{NAG}: {Network} for {Adversary} {Generation} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8578182},
	urldate = {2020-01-16},
	file = {NAG\: Network for Adversary Generation - IEEE Conference Publication:/Users/maumau/Zotero/storage/SWJKR673/8578182.html:text/html}
}

@misc{noauthor_epat_nodate,
	title = {{EPAT}: {Euclidean} {Perturbation} {Analysis} and {Transform} - {An} {Agnostic} {Data} {Adaptation} {Framework} for {Improving} {Facial} {Landmark} {Detectors} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/7961745},
	urldate = {2020-01-16},
	file = {EPAT\: Euclidean Perturbation Analysis and Transform - An Agnostic Data Adaptation Framework for Improving Facial Landmark Detectors - IEEE Conference Publication:/Users/maumau/Zotero/storage/MIFVRRMD/7961745.html:text/html}
}

@misc{noauthor_universal_nodate,
	title = {Universal {Adversarial} {Perturbations} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8099500},
	urldate = {2020-01-16},
	file = {Universal Adversarial Perturbations - IEEE Conference Publication:/Users/maumau/Zotero/storage/UAH549U4/8099500.html:text/html}
}

@misc{noauthor_early_nodate,
	title = {Early stabilizing feature importance for {TensorFlow} deep neural networks - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/7966442},
	urldate = {2020-01-16},
	file = {Early stabilizing feature importance for TensorFlow deep neural networks - IEEE Conference Publication:/Users/maumau/Zotero/storage/NMGWZ5EC/7966442.html:text/html}
}

@misc{noauthor_time-domain_nodate,
	title = {Time-{Domain} {Modeling} of {All}-{Digital} {PLLs} to {Single}-{Event} {Upset} {Perturbations} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8103794},
	urldate = {2020-01-16},
	file = {Time-Domain Modeling of All-Digital PLLs to Single-Event Upset Perturbations - IEEE Journals & Magazine:/Users/maumau/Zotero/storage/QBH3EX4V/8103794.html:text/html}
}

@misc{noauthor_universal_nodate-1,
	title = {Universal {Adversarial} {Perturbations} {Against} {Semantic} {Image} {Segmentation} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8237562},
	urldate = {2020-01-16},
	file = {Universal Adversarial Perturbations Against Semantic Image Segmentation - IEEE Conference Publication:/Users/maumau/Zotero/storage/IWM6W8U3/8237562.html:text/html}
}

@misc{noauthor_art_nodate,
	title = {Art of {Singular} {Vectors} and {Universal} {Adversarial} {Perturbations} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8578991},
	urldate = {2020-01-16},
	file = {Art of Singular Vectors and Universal Adversarial Perturbations - IEEE Conference Publication:/Users/maumau/Zotero/storage/T3JDK6AY/8578991.html:text/html}
}

@misc{noauthor_interpretable_nodate,
	title = {Interpretable {Explanations} of {Black} {Boxes} by {Meaningful} {Perturbation} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8237633},
	urldate = {2020-01-16},
	file = {Interpretable Explanations of Black Boxes by Meaningful Perturbation - IEEE Conference Publication:/Users/maumau/Zotero/storage/UCQN7FD6/8237633.html:text/html}
}

@misc{noauthor_universal_nodate-2,
	title = {Universal {Rules} for {Fooling} {Deep} {Neural} {Networks} based {Text} {Classification} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8790213},
	urldate = {2020-01-16},
	file = {Universal Rules for Fooling Deep Neural Networks based Text Classification - IEEE Conference Publication:/Users/maumau/Zotero/storage/8N5P5PF6/8790213.html:text/html}
}

@misc{noauthor_network_nodate,
	title = {Network {Anomaly} {Detection} {Using} a {Commute} {Distance} {Based} {Approach} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/5693397},
	urldate = {2020-01-16},
	file = {Network Anomaly Detection Using a Commute Distance Based Approach - IEEE Conference Publication:/Users/maumau/Zotero/storage/KQH8P28S/5693397.html:text/html}
}

@misc{noauthor_image_nodate-1,
	title = {Image {Super}-{Resolution} as a {Defense} {Against} {Adversarial} {Attacks} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8844865},
	urldate = {2020-01-16},
	file = {Image Super-Resolution as a Defense Against Adversarial Attacks - IEEE Journals & Magazine:/Users/maumau/Zotero/storage/SWC3IIHH/8844865.html:text/html}
}

@misc{noauthor_robust_nodate,
	title = {Robust {Estimator}-{Correlator} for {Spectrum} {Sensing} in {MIMO} {CR} {Networks} with {CSI} {Uncertainty} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/6751601},
	urldate = {2020-01-16},
	file = {Robust Estimator-Correlator for Spectrum Sensing in MIMO CR Networks with CSI Uncertainty - IEEE Journals & Magazine:/Users/maumau/Zotero/storage/TNN7YIWD/6751601.html:text/html}
}

@misc{noauthor_training_nodate,
	title = {Training {Deep} {Neural} {Networks} for {Visual} {Servoing} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8461068},
	urldate = {2020-01-16},
	file = {Training Deep Neural Networks for Visual Servoing - IEEE Conference Publication:/Users/maumau/Zotero/storage/BYI2AAGV/8461068.html:text/html}
}

@misc{noauthor_deflecting_nodate,
	title = {Deflecting {Adversarial} {Attacks} with {Pixel} {Deflection} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8578992},
	urldate = {2020-01-16},
	file = {Deflecting Adversarial Attacks with Pixel Deflection - IEEE Conference Publication:/Users/maumau/Zotero/storage/RTJIDPDF/8578992.html:text/html}
}

@misc{noauthor_feature_nodate,
	title = {Feature {Space} {Perturbations} {Yield} {More} {Transferable} {Adversarial} {Examples} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8953700},
	urldate = {2020-01-16},
	file = {Feature Space Perturbations Yield More Transferable Adversarial Examples - IEEE Conference Publication:/Users/maumau/Zotero/storage/GTXDD4TL/8953700.html:text/html}
}

@misc{noauthor_structural_nodate,
	title = {On the {Structural} {Sensitivity} of {Deep} {Convolutional} {Networks} to the {Directions} of {Fourier} {Basis} {Functions} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8954086},
	urldate = {2020-01-16},
	file = {On the Structural Sensitivity of Deep Convolutional Networks to the Directions of Fourier Basis Functions - IEEE Conference Publication:/Users/maumau/Zotero/storage/PGQN7BZW/8954086.html:text/html}
}

@misc{noauthor_adversarial_nodate,
	title = {Adversarial {Defense} by {Stratified} {Convolutional} {Sparse} {Coding} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8953552},
	urldate = {2020-01-16},
	file = {Adversarial Defense by Stratified Convolutional Sparse Coding - IEEE Conference Publication:/Users/maumau/Zotero/storage/FS2E3K8P/8953552.html:text/html}
}

@misc{noauthor_survey_nodate,
	title = {A {Survey} on {Security} {Threats} and {Defensive} {Techniques} of {Machine} {Learning}: {A} {Data} {Driven} {View} - {IEEE} {Journals} \& {Magazine}},
	url = {https://ieeexplore-ieee-org.proxy2.biblio.supsi.ch/document/8290925},
	urldate = {2020-01-16},
	file = {A Survey on Security Threats and Defensive Techniques of Machine Learning\: A Data Driven View - IEEE Journals & Magazine:/Users/maumau/Zotero/storage/S9AA82B5/8290925.html:text/html}
}

@inproceedings{oh_adversarial_2017,
	title = {Adversarial {Image} {Perturbation} for {Privacy} {Protection} {A} {Game} {Theory} {Perspective}},
	doi = {10.1109/ICCV.2017.165},
	abstract = {Users like sharing personal photos with others through social media. At the same time, they might want to make automatic identification in such photos difficult or even impossible. Classic obfuscation methods such as blurring are not only unpleasant but also not as effective as one would expect [28, 37, 18]. Recent studies on adversarial image perturbations (AIP) suggest that it is possible to confuse recognition systems effectively without unpleasant artifacts. However, in the presence of counter measures against AIPs [7], it is unclear how effective AIP would be in particular when the choice of counter measure is unknown. Game theory provides tools for studying the interaction between agents with uncertainties in the strategies. We introduce a general game theoretical framework for the user-recogniser dynamics, and present a case study that involves current state of the art AIP and person recognition techniques. We derive the optimal strategy for the user that assures an upper bound on the recognition rate independent of the recogniser's counter measure. Code is available at https://goo.gl/hgvbNK.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Oh, Seong Joon and Fritz, Mario and Schiele, Bernt},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Games, Robustness, Perturbation methods, adversarial image perturbation, automatic identification, data protection, game theory, Game theory, Identification of persons, image processing tactics, image recognition, person recognition, personal photo sharing, Privacy, privacy protection, social media, Social network services, social networking (online)},
	pages = {1491--1500},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/7XI45FZ6/8237427.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/LIG7IQR6/Oh et al. - 2017 - Adversarial Image Perturbation for Privacy Protect.pdf:application/pdf}
}

@article{fawzi_robustness_2017-1,
	title = {The {Robustness} of {Deep} {Networks}: {A} {Geometrical} {Perspective}},
	volume = {34},
	issn = {1558-0792},
	shorttitle = {The {Robustness} of {Deep} {Networks}},
	doi = {10.1109/MSP.2017.2740965},
	abstract = {Deep neural networks have recently shown impressive classification performance on a diverse set of visual tasks. When deployed in real-world (noise-prone) environments, it is equally important that these classifiers satisfy robustness guarantees: small perturbations applied to the samples should not yield significant loss to the performance of the predictor. The goal of this article is to discuss the robustness of deep networks to a diverse set of perturbations that may affect the samples in practice, including adversarial perturbations, random noise, and geometric transformations. This article further discusses the recent works that build on the robustness analysis to provide geometric insights on the classifier's decision surface, which help in developing a better understanding of deep networks. Finally, we present recent solutions that attempt to increase the robustness of deep networks. We hope this review article will contribute to shedding light on the open research challenges in the robustness of deep networks and stir interest in the analysis of their fundamental properties.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	month = nov,
	year = {2017},
	keywords = {neural nets, deep neural networks, Neural networks, Machine learning, Robustness, image classification, adversarial perturbations, Classification, classifier decision surface, geometric transformations, geometrical perspective, impressive classification performance, noise-prone environments, predictor performance, random noise, real-world environments, robustness analysis, visual tasks, Visualization},
	pages = {50--62},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/Z9EW3JAG/8103145.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/NANFVG4H/Fawzi et al. - 2017 - The Robustness of Deep Networks A Geometrical Per.pdf:application/pdf}
}

@article{akhtar_threat_2018-1,
	title = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}: {A} {Survey}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Threat of {Adversarial} {Attacks} on {Deep} {Learning} in {Computer} {Vision}},
	doi = {10.1109/ACCESS.2018.2807385},
	abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	journal = {IEEE Access},
	author = {Akhtar, Naveed and Mian, Ajmal},
	year = {2018},
	keywords = {learning (artificial intelligence), neural nets, deep neural networks, Deep learning, Neural networks, Computational modeling, Predictive models, Machine learning, Computer vision, Perturbation methods, Task analysis, artificial intelligence, adversarial attacks, adversarial learning, adversarial perturbation, black-box attack, computer vision, deep learning models, humanities, perturbation detection, self-driving cars, white-box attack},
	pages = {14410--14430},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/N4MI4UI5/8294186.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/Q3A9M44B/Akhtar and Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf:application/pdf}
}

@inproceedings{min_adversarial_2018-1,
	title = {Adversarial {Attack}? {Don}'t {Panic}},
	shorttitle = {Adversarial {Attack}?},
	doi = {10.1109/BIGCOM.2018.00021},
	abstract = {Deep learning is playing a more and more important role in our daily life and scientific research such as autonomous systems, intelligent life and data mining. However, numerous studies have showed that deep learning with superior performance on many tasks may suffer from subtle perturbations constructed by attacker purposely, called adversarial perturbations, which are imperceptible to human observers but completely effect deep neural network models. The emergence of adversarial attacks has led to questions about neural networks. Therefore, machine learning security and privacy are becoming an increasingly active research area. In this paper, we summarize the prevalent methods for the generating adversarial attacks according to three groups. We elaborated on their ideas and principles of generation. We further analyze the common limitations of these methods and implement statistical experiments of the last layer output on CleverHans to reveal that the detection of adversarial samples is not as difficult as it seems and can be achieved in some relatively simple manners.},
	booktitle = {2018 4th {International} {Conference} on {Big} {Data} {Computing} and {Communications} ({BIGCOM})},
	author = {Min, Feixia and Qiu, Xiaofeng and Wu, Fan},
	month = aug,
	year = {2018},
	note = {ISSN: null},
	keywords = {learning (artificial intelligence), neural nets, deep learning, Neural networks, security of data, data mining, Machine learning, Machine learning algorithms, Perturbation methods, Manifolds, adversarial perturbations, adversarial attack, autonomous systems, Classification algorithms, data privacy, deep learning, adversarial attacks, adversarial generation algorithms, easy detection, deep neural network models, intelligent life, Iterative methods, machine learning privacy, machine learning security},
	pages = {90--95},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/NLPLIXMD/8488630.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/NZVJ436C/Min et al. - 2018 - Adversarial Attack Don't Panic.pdf:application/pdf}
}

@inproceedings{qiao_toward_2018,
	title = {Toward {Intelligent} {Detection} {Modelling} for {Adversarial} {Samples} in {Convolutional} {Neural} {Networks}},
	doi = {10.1109/CAMAD.2018.8514982},
	abstract = {Deep Neural Networks (DNNs) are hierarchical nonlinear architectures that have been widely used in artificial intelligence applications. However, these models are vulnerable to adversarial perturbations which add changes slightly and are crafted explicitly to fool the model. Such attacks will cause the neural network to completely change its classification of data. Although various defense strategies have been proposed, existing defense methods have two limitations. First, the discovery success rate is not very high. Second, existing methods depend on the output of a particular layer in a specific learning structure. In this paper, we propose a powerful method for adversarial samples using Large Margin Cosine Estimate(LMCE). By iteratively calculating the large-margin cosine uncertainty estimates between the model predictions, the results can be regarded as a novel measurement of model uncertainty estimation and is available to detect adversarial samples by training using a simple machine learning algorithm. Comparing it with the way in which adversar- ial samples are generated, it is confirmed that this measurement can better distinguish hostile disturbances. We modeled deep neural network attacks and established defense mechanisms against various types of adversarial attacks. Classifier gets better performance than the baseline model. The approach is validated on a series of standard datasets including MNIST and CIFAR -10, outperforming previous ensemble method with strong statistical significance. Experiments indicate that our approach generalizes better across different architectures and attacks.},
	booktitle = {2018 {IEEE} 23rd {International} {Workshop} on {Computer} {Aided} {Modeling} and {Design} of {Communication} {Links} and {Networks} ({CAMAD})},
	author = {Qiao, Zhuobiao and Dong, Mianxiong and Ota, Kaoru and Wu, Jun},
	month = sep,
	year = {2018},
	note = {ISSN: 2378-4865},
	keywords = {learning (artificial intelligence), convolution, deep neural networks, feedforward neural nets, Training, Neural networks, Computational modeling, statistical analysis, Perturbation methods, artificial intelligence, Manifolds, Speech recognition, pattern classification, adversarial samples, adversarial perturbations, Adversarial samples, artificial intelligence applications, CNN attacks and detec- tion, convolutional neural networks, data classification, DNN, ensemble method, hierarchical nonlinear architectures, intelligent detection modelling, Large Margin Cosine Estimate, large-margin cosine uncertainty, LMCE, model uncertainty estimation, simple machine learning algorithm, statistical significance, Uncertainty, uncertainty handling},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/HXL9D9V8/8514982.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/EZU9VTVB/Qiao et al. - 2018 - Toward Intelligent Detection Modelling for Adversa.pdf:application/pdf}
}

@inproceedings{wang_universal_2018,
	title = {Universal {Perturbation} {Generation} for {Black}-box {Attack} {Using} {Evolutionary} {Algorithms}},
	doi = {10.1109/ICPR.2018.8546023},
	abstract = {Image classifiers based on deep neural networks (DNNs) are vulnerable to tiny, imperceptible perturbations. Maliciously generated adversarial examples can exploit the instability of DNNs and mislead it into outputting a wrong classification result. Prior works showed the transferability of adversarial perturbations between models and between images. In this work, we shed light on the combination of source/target misclassification, black-box attack, and universal perturbation by employing improved evolutionary algorithms. We additionally find that the use of adversarial initialization enhances the efficiency of evolutionary algorithms finding universal perturbations. Experiments demonstrate impressive misclassification rates and surprising transferability for the proposed attack method using different models trained on CIFAR-10 and CIFAR-100 datasets. Our attach method also shows robustness against defensive measures like adversarial training.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Wang, Sivy and Shi, Yucheng and Han, Yahong},
	month = aug,
	year = {2018},
	note = {ISSN: 1051-4651},
	keywords = {neural nets, deep neural networks, Training, Neural networks, feature extraction, Robustness, image classification, Perturbation methods, adversarial training, black-box attack, adversarial perturbations, adversarial initialization, evolutionary algorithms, evolutionary computation, Evolutionary computation, image classifiers, Sociology, source misclassification, Statistics, target misclassification, universal perturbation generation},
	pages = {1277--1282},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/GDQHEYEB/8546023.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/ACVNZJXN/Wang et al. - 2018 - Universal Perturbation Generation for Black-box At.pdf:application/pdf}
}

@inproceedings{orekondy_connecting_2018,
	title = {Connecting {Pixels} to {Privacy} and {Utility}: {Automatic} {Redaction} of {Private} {Information} in {Images}},
	shorttitle = {Connecting {Pixels} to {Privacy} and {Utility}},
	doi = {10.1109/CVPR.2018.00883},
	abstract = {Images convey a broad spectrum of personal information. If such images are shared on social media platforms, this personal information is leaked which conflicts with the privacy of depicted persons. Therefore, we aim for automated approaches to redact such private information and thereby protect privacy of the individual. By conducting a user study we find that obfuscating the image regions related to the private information leads to privacy while retaining utility of the images. Moreover, by varying the size of the regions different privacy-utility trade-offs can be achieved. Our findings argue for a "redaction by segmentation" paradigm. Hence, we propose the first sizable dataset of private images "in the wild" annotated with pixel and instance level labels across a broad range of privacy classes. We present the first model for automatic redaction of diverse private information. It is effective at achieving various privacy-utility trade-offs within 83\% of the performance of redactions based on ground-truth annotation.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Orekondy, Tribhuvanesh and Fritz, Mario and Schiele, Bernt},
	month = jun,
	year = {2018},
	note = {ISSN: 1063-6919},
	keywords = {Face, Task analysis, Visualization, data privacy, Privacy, privacy protection, social networking (online), automatic redaction, Data privacy, Fingerprint recognition, ground-truth annotation, image regions, image segmentation, Image segmentation, instance level labels, personal information, pixel connection, privacy classes, privacy-utility trade-offs, private images segmentation, private information, social media platforms},
	pages = {8466--8475},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/JVXTYM4S/8578981.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/HR7HK6BD/Orekondy et al. - 2018 - Connecting Pixels to Privacy and Utility Automati.pdf:application/pdf}
}

@inproceedings{dong_boosting_2018,
	title = {Boosting {Adversarial} {Attacks} with {Momentum}},
	doi = {10.1109/CVPR.2018.00957},
	abstract = {Deep neural networks are vulnerable to adversarial examples, which poses security concerns on these algorithms due to the potentially severe consequences. Adversarial attacks serve as an important surrogate to evaluate the robustness of deep learning models before they are deployed. However, most of existing adversarial attacks can only fool a black-box model with a low success rate. To address this issue, we propose a broad class of momentum-based iterative algorithms to boost adversarial attacks. By integrating the momentum term into the iterative process for attacks, our methods can stabilize update directions and escape from poor local maxima during the iterations, resulting in more transferable adversarial examples. To further improve the success rates for black-box attacks, we apply momentum iterative algorithms to an ensemble of models, and show that the adversarially trained models with a strong defense ability are also vulnerable to our black-box attacks. We hope that the proposed methods will serve as a benchmark for evaluating the robustness of various deep models and defense methods. With this method, we won the first places in NIPS 2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack competitions.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Dong, Yinpeng and Liao, Fangzhou and Pang, Tianyu and Su, Hang and Zhu, Jun and Hu, Xiaolin and Li, Jianguo},
	month = jun,
	year = {2018},
	note = {ISSN: 1063-6919},
	keywords = {neural nets, deep neural networks, Training, Data models, security of data, Robustness, Security, deep learning models, Iterative methods, Adaptation models, adversarially trained models, black-box attacks, black-box model, boosting adversarial attacks, defense methods, iterative methods, momentum iterative algorithms},
	pages = {9185--9193},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/4CZYHQC9/8579055.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/V96YJRQM/Dong et al. - 2018 - Boosting Adversarial Attacks with Momentum.pdf:application/pdf}
}

@inproceedings{akhtar_defense_2018-2,
	title = {Defense {Against} {Universal} {Adversarial} {Perturbations}},
	doi = {10.1109/CVPR.2018.00357},
	abstract = {Recent advances in Deep Learning show the existence of image-agnostic quasi-imperceptible perturbations that when applied to 'any' image can fool a state-of-the-art network classifier to change its prediction about the image label. These 'Universal Adversarial Perturbations' pose a serious threat to the success of Deep Learning in practice. We present the first dedicated framework to effectively defend the networks against such perturbations. Our approach learns a Perturbation Rectifying Network (PRN) as 'pre-input' layers to a targeted model, such that the targeted model needs no modification. The PRN is learned from real and synthetic image-agnostic perturbations, where an efficient method to compute the latter is also proposed. A perturbation detector is separately trained on the Discrete Cosine Transform of the input-output difference of the PRN. A query image is first passed through the PRN and verified by the detector. If a perturbation is detected, the output of the PRN is used for label prediction instead of the actual image. A rigorous evaluation shows that our framework can defend the network classifiers against unseen adversarial perturbations in the real-world scenarios with up to 97.5\% success rate. The PRN also generalizes well in the sense that training for one targeted network defends another network with a comparable success rate.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Akhtar, Naveed and Liu, Jian and Mian, Ajmal},
	month = jun,
	year = {2018},
	note = {ISSN: 1063-6919},
	keywords = {learning (artificial intelligence), deep learning, Training, Neural networks, Computational modeling, Robustness, Perturbation methods, Detectors, Integrated circuits, pattern classification, image processing, discrete cosine transform, discrete cosine transforms, image label, label prediction, network classifiers, perturbation detector, perturbation rectifying network, PRN, query image, synthetic image-agnostic perturbations, targeted network, universal adversarial perturbations},
	pages = {3389--3398},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/PQWQUJ7A/8578455.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/89DAPTS9/Akhtar et al. - 2018 - Defense Against Universal Adversarial Perturbation.pdf:application/pdf}
}

@inproceedings{ye_hardware_2018,
	title = {Hardware {Trojan} in {FPGA} {CNN} {Accelerator}},
	doi = {10.1109/ATS.2018.00024},
	abstract = {Maliciously manipulating prediction results of Convolutional Neural Network (CNN) is a severe security threat. Previous works studied this threat from the aspects of dataset and model. However, with the increasing developments of CNN accelerators nowadays, the role of hardware in this threat lacks attentions. This paper inserts a hardware Trojan into the convolutional operations of a FPGA CNN accelerator. The experiments on ImageNet show that, with only 0.0051\% hardware overhead to the accelerator and 0.000356\% modification to an image, the hardware Trojan can be triggered to 100\% precisely control the CNN classification result of the image.},
	booktitle = {2018 {IEEE} 27th {Asian} {Test} {Symposium} ({ATS})},
	author = {Ye, Jing and Hu, Yu and Li, Xiaowei},
	month = oct,
	year = {2018},
	note = {ISSN: 1081-7735},
	keywords = {invasive software, CNN accelerators, CNN classification result, CNN, Accelerator, FPGA, Hardware Trojan, convolutional neural nets, convolutional neural network, convolutional operations, field programmable gate arrays, Field programmable gate arrays, FPGA CNN accelerator, Hardware, hardware overhead, hardware Trojan, ImageNet, Indexes, Manganese, prediction results, severe security threat, Trojan horses},
	pages = {68--73},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/6L7QIY57/8567413.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/XJGX724G/Ye et al. - 2018 - Hardware Trojan in FPGA CNN Accelerator.pdf:application/pdf}
}

@inproceedings{gast_lightweight_2018,
	title = {Lightweight {Probabilistic} {Deep} {Networks}},
	doi = {10.1109/CVPR.2018.00355},
	abstract = {Even though probabilistic treatments of neural networks have a long history, they have not found widespread use in practice. Sampling approaches are often too slow already for simple networks. The size of the inputs and the depth of typical CNN architectures in computer vision only compound this problem. Uncertainty in neural networks has thus been largely ignored in practice, despite the fact that it may provide important information about the reliability of predictions and the inner workings of the network. In this paper, we introduce two lightweight approaches to making supervised learning with probabilistic deep networks practical: First, we suggest probabilistic output layers for classification and regression that require only minimal changes to existing networks. Second, we employ assumed density filtering and show that activation uncertainties can be propagated in a practical fashion through the entire network, again with minor changes. Both probabilistic networks retain the predictive power of the deterministic counterpart, but yield uncertainties that correlate well with the empirical error induced by their predictions. Moreover, the robustness to adversarial examples is significantly increased.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Gast, Jochen and Roth, Stefan},
	month = jun,
	year = {2018},
	note = {ISSN: 1063-6919},
	keywords = {learning (artificial intelligence), convolution, feedforward neural nets, Neural networks, probability, regression analysis, Computer architecture, pattern classification, Bayes methods, Uncertainty, classification, CNN architectures, deep convolutional networks, lightweight probabilistic deep networks, neural net architecture, neural network uncertainty, Probabilistic logic, probabilistic output layers, regression, Standards, supervised learning, Supervised learning},
	pages = {3369--3378},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/BPU2N9M3/8578453.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/8B22LIWA/Gast and Roth - 2018 - Lightweight Probabilistic Deep Networks.pdf:application/pdf}
}

@inproceedings{xu_fooling_2018,
	title = {Fooling {Vision} and {Language} {Models} {Despite} {Localization} and {Attention} {Mechanism}},
	doi = {10.1109/CVPR.2018.00520},
	abstract = {Adversarial attacks are known to succeed on classifiers, but it has been an open question whether more complex vision systems are vulnerable. In this paper, we study adversarial examples for vision and language models, which incorporate natural language understanding and complex structures such as attention, localization, and modular architectures. In particular, we investigate attacks on a dense captioning model and on two visual question answering (VQA) models. Our evaluation shows that we can generate adversarial examples with a high success rate (i.e., {\textgreater} 90\%) for these models. Our work sheds new light on understanding adversarial attacks on vision systems which have a language component and shows that attention, bounding box localization, and compositional internal structures are vulnerable to adversarial attacks. These observations will inform future work towards building effective defenses.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Xu, Xiaojun and Chen, Xinyun and Liu, Chang and Rohrbach, Anna and Darrell, Trevor and Song, Dawn},
	month = jun,
	year = {2018},
	note = {ISSN: 1063-6919},
	keywords = {learning (artificial intelligence), Neural networks, Computational modeling, security of data, machine learning, Predictive models, Prediction algorithms, image classification, adversarial attacks, computer vision, image retrieval, natural language processing, Visualization, attention mechanism, box localization, complex structures, complex vision systems, dense captioning model, fooling vision, Knowledge discovery, language component, language models, localization, natural language understanding, Natural languages, question answering (information retrieval), visual question answering models},
	pages = {4951--4961},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/CUSZ99NE/8578618.html:text/html;Submitted Version:/Users/maumau/Zotero/storage/9H5B53TN/Xu et al. - 2018 - Fooling Vision and Language Models Despite Localiz.pdf:application/pdf}
}

@inproceedings{su_empirical_2018,
	address = {Takayama},
	title = {Empirical {Evaluation} on {Robustness} of {Deep} {Convolutional} {Neural} {Networks} {Activation} {Functions} {Against} {Adversarial} {Perturbation}},
	isbn = {978-1-5386-9184-7},
	url = {https://ieeexplore.ieee.org/document/8590903/},
	doi = {10.1109/CANDARW.2018.00049},
	urldate = {2020-01-16},
	booktitle = {2018 {Sixth} {International} {Symposium} on {Computing} and {Networking} {Workshops} ({CANDARW})},
	publisher = {IEEE},
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
	month = nov,
	year = {2018},
	pages = {223--227}
}

@inproceedings{arnab_robustness_2018,
	address = {Salt Lake City, UT},
	title = {On the {Robustness} of {Semantic} {Segmentation} {Models} to {Adversarial} {Attacks}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578197/},
	doi = {10.1109/CVPR.2018.00099},
	urldate = {2020-01-16},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Arnab, Anurag and Miksik, Ondrej and Torr, Philip H.S.},
	month = jun,
	year = {2018},
	pages = {888--897},
	file = {Submitted Version:/Users/maumau/Zotero/storage/3QW8YHUD/Arnab et al. - 2018 - On the Robustness of Semantic Segmentation Models .pdf:application/pdf}
}
 
@inproceedings{Gundersen2018StateOT,
	author={Odd Erik Gundersen and Sigbj{\o}rn Kjensmo},
	title = {State of the Art: Reproducibility in Artificial Intelligence},
	conference = {Thirty-Second AAAI Conference on Artificial Intelligence},
	year = {2018},
  booktitle={AAAI Publications},
	keywords = {empirical research; reproducible research; research method; documentation},
	abstract = {Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20% and 30% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with in- creased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.}
	
}

@article{xue_defeating_2019,
	title = {Defeating {Untrustworthy} {Testing} {Parties}: {A} {Novel} {Hybrid} {Clustering} {Ensemble} {Based} {Golden} {Models}-{Free} {Hardware} {Trojan} {Detection} {Method}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Defeating {Untrustworthy} {Testing} {Parties}},
	url = {https://ieeexplore.ieee.org/document/8580552/},
	doi = {10.1109/ACCESS.2018.2887268},
	urldate = {2020-01-16},
	journal = {IEEE Access},
	author = {Xue, Mingfu and Bian, Rongzhen and Liu, Weiqiang and Wang, Jian},
	year = {2019},
	pages = {5124--5140},
	file = {Full Text:/Users/maumau/Zotero/storage/SMJ86Y7H/Xue et al. - 2019 - Defeating Untrustworthy Testing Parties A Novel H.pdf:application/pdf}
}

@inproceedings{clements_backdoor_2018,
	address = {Anaheim, CA, USA},
	title = {{BACKDOOR} {ATTACKS} {ON} {NEURAL} {NETWORK} {OPERATIONS}},
	isbn = {978-1-72811-295-4},
	url = {https://ieeexplore.ieee.org/document/8646335/},
	doi = {10.1109/GlobalSIP.2018.8646335},
	urldate = {2020-01-16},
	booktitle = {2018 {IEEE} {Global} {Conference} on {Signal} and {Information} {Processing} ({GlobalSIP})},
	publisher = {IEEE},
	author = {Clements, Joseph and Lao, Yingjie},
	month = nov,
	year = {2018},
	pages = {1154--1158}
}

@inproceedings{zhang_effect_2018,
	address = {Zhengzhou, China},
	title = {Effect of {Adversarial} {Examples} on the {Robustness} of {CAPTCHA}},
	isbn = {978-1-72810-974-9},
	url = {https://ieeexplore.ieee.org/document/8644665/},
	doi = {10.1109/CyberC.2018.00013},
	urldate = {2020-01-16},
	booktitle = {2018 {International} {Conference} on {Cyber}-{Enabled} {Distributed} {Computing} and {Knowledge} {Discovery} ({CyberC})},
	publisher = {IEEE},
	author = {Zhang, Yang and Gao, Haichang and Pei, Ge and Kang, Shuai and Zhou, Xin},
	month = oct,
	year = {2018},
	pages = {1--109}
}

@article{sadeghi_adversarial_2019,
	title = {Adversarial {Attacks} on {Deep}-{Learning} {Based} {Radio} {Signal} {Classification}},
	volume = {8},
	issn = {2162-2337, 2162-2345},
	url = {https://ieeexplore.ieee.org/document/8449065/},
	doi = {10.1109/LWC.2018.2867459},
	number = {1},
	urldate = {2020-01-16},
	journal = {IEEE Wireless Communications Letters},
	author = {Sadeghi, Meysam and Larsson, Erik G.},
	month = feb,
	year = {2019},
	pages = {213--216}
}

@inproceedings{pittaluga_learning_2019,
	address = {Waikoloa Village, HI, USA},
	title = {Learning {Privacy} {Preserving} {Encodings} {Through} {Adversarial} {Training}},
	isbn = {978-1-72811-975-5},
	url = {https://ieeexplore.ieee.org/document/8658803/},
	doi = {10.1109/WACV.2019.00089},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Pittaluga, Francesco and Koppal, Sanjeev and Chakrabarti, Ayan},
	month = jan,
	year = {2019},
	pages = {791--799}
}

@article{pan_recent_2019,
	title = {Recent {Progress} on {Generative} {Adversarial} {Networks} ({GANs}): {A} {Survey}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Recent {Progress} on {Generative} {Adversarial} {Networks} ({GANs})},
	url = {https://ieeexplore.ieee.org/document/8667290/},
	doi = {10.1109/ACCESS.2019.2905015},
	urldate = {2020-01-16},
	journal = {IEEE Access},
	author = {Pan, Zhaoqing and Yu, Weijie and Yi, Xiaokai and Khan, Asifullah and Yuan, Feng and Zheng, Yuhui},
	year = {2019},
	pages = {36322--36333}
}

@article{singh_recognizing_2019,
	title = {Recognizing {Disguised} {Faces} in the {Wild}},
	volume = {1},
	issn = {2637-6407},
	url = {https://ieeexplore.ieee.org/document/8663389/},
	doi = {10.1109/TBIOM.2019.2903860},
	number = {2},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Biometrics, Behavior, and Identity Science},
	author = {Singh, Maneet and Singh, Richa and Vatsa, Mayank and Ratha, Nalini K. and Chellappa, Rama},
	month = apr,
	year = {2019},
	pages = {97--108}
}

@inproceedings{sheikholeslami_efficient_2019,
	address = {Brighton, United Kingdom},
	title = {Efficient {Randomized} {Defense} against {Adversarial} {Attacks} in {Deep} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8683348/},
	doi = {10.1109/ICASSP.2019.8683348},
	urldate = {2020-01-16},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Sheikholeslami, Fatemeh and Jain, Swayambhoo and Giannakis, Georgios B.},
	month = may,
	year = {2019},
	pages = {3277--3281}
}

@inproceedings{goel_smartbox_2018,
	address = {Redondo Beach, CA, USA},
	title = {{SmartBox}: {Benchmarking} {Adversarial} {Detection} and {Mitigation} {Algorithms} for {Face} {Recognition}},
	isbn = {978-1-5386-7180-1},
	shorttitle = {{SmartBox}},
	url = {https://ieeexplore.ieee.org/document/8698567/},
	doi = {10.1109/BTAS.2018.8698567},
	urldate = {2020-01-16},
	booktitle = {2018 {IEEE} 9th {International} {Conference} on {Biometrics} {Theory}, {Applications} and {Systems} ({BTAS})},
	publisher = {IEEE},
	author = {Goel, Akhil and Singh, Anirudh and Agarwal, Akshay and Vatsa, Mayank and Singh, Richa},
	month = oct,
	year = {2018},
	pages = {1--7}
}

@inproceedings{huang_perspective_2018,
	address = {Redondo Beach, CA, USA},
	title = {From the {Perspective} of {CNN} to {Adversarial} {Iris} {Images}},
	isbn = {978-1-5386-7180-1},
	url = {https://ieeexplore.ieee.org/document/8698562/},
	doi = {10.1109/BTAS.2018.8698562},
	urldate = {2020-01-16},
	booktitle = {2018 {IEEE} 9th {International} {Conference} on {Biometrics} {Theory}, {Applications} and {Systems} ({BTAS})},
	publisher = {IEEE},
	author = {Huang, Yi and Kong Wai-Kin, Adams and Lam, Kwok-Yan},
	month = oct,
	year = {2018},
	pages = {1--10}
}

@inproceedings{behjati_universal_2019,
	address = {Brighton, United Kingdom},
	title = {Universal {Adversarial} {Attacks} on {Text} {Classifiers}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682430/},
	doi = {10.1109/ICASSP.2019.8682430},
	urldate = {2020-01-16},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Behjati, Melika and Moosavi-Dezfooli, Seyed-Mohsen and Baghshah, Mahdieh Soleymani and Frossard, Pascal},
	month = may,
	year = {2019},
	pages = {7345--7349}
}

@inproceedings{li_scene_2019,
	address = {Brighton, United Kingdom},
	title = {Scene {Privacy} {Protection}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682225/},
	doi = {10.1109/ICASSP.2019.8682225},
	urldate = {2020-01-16},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Li, Chau Yi and Shahin Shamsabadi, Ali and Sanchez-Matilla, Ricardo and Mazzon, Riccardo and Cavallaro, Andrea},
	month = may,
	year = {2019},
	pages = {2502--2506}
}

@inproceedings{agarwal_are_2018,
	address = {Redondo Beach, CA, USA},
	title = {Are {Image}-{Agnostic} {Universal} {Adversarial} {Perturbations} for {Face} {Recognition} {Difficult} to {Detect}?},
	isbn = {978-1-5386-7180-1},
	url = {https://ieeexplore.ieee.org/document/8698548/},
	doi = {10.1109/BTAS.2018.8698548},
	urldate = {2020-01-16},
	booktitle = {2018 {IEEE} 9th {International} {Conference} on {Biometrics} {Theory}, {Applications} and {Systems} ({BTAS})},
	publisher = {IEEE},
	author = {Agarwal, Akshay and Singh, Richa and Vatsa, Mayank and Ratha, Nalini},
	month = oct,
	year = {2018},
	pages = {1--7}
}

@article{sadeghi_physical_2019,
	title = {Physical {Adversarial} {Attacks} {Against} {End}-to-{End} {Autoencoder} {Communication} {Systems}},
	volume = {23},
	issn = {1089-7798, 1558-2558, 2373-7891},
	url = {https://ieeexplore.ieee.org/document/8651357/},
	doi = {10.1109/LCOMM.2019.2901469},
	number = {5},
	urldate = {2020-01-16},
	journal = {IEEE Communications Letters},
	author = {Sadeghi, Meysam and Larsson, Erik G.},
	month = may,
	year = {2019},
	pages = {847--850},
	file = {Full Text:/Users/maumau/Zotero/storage/UKTDPDFW/Sadeghi and Larsson - 2019 - Physical Adversarial Attacks Against End-to-End Au.pdf:application/pdf}
}
@inproceedings{Szegedy_2014,
title = {Intriguing properties of neural networks.},
author	= {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
publisher = {ICLR},
year =  {2014},
url = {URL http://arxiv.org/abs/1312.6199},
booktitle	= {International Conference on Learning Representations}
}


@inproceedings{goodfellow_2014,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6572},
booktitle	= {International Conference on Learning Representations}
}

@article{REN2020346,
title = "Adversarial Attacks and Defenses in Deep Learning",
journal = "Engineering",
volume = "6",
number = "3",
pages = "346 - 360",
year = "2020",
issn = "2095-8099",
doi = "https://doi.org/10.1016/j.eng.2019.12.012",
url = "http://www.sciencedirect.com/science/article/pii/S209580991930503X",
author = "Kui Ren and Tianhang Zheng and Zhan Qin and Xue Liu",
keywords = "Machine learning, Deep neural network, Adversarial example, Adversarial attack, Adversarial defense",
abstract = "With the rapid developments of artificial intelligence (AI) and deep learning (DL) techniques, it is critical to ensure the security and robustness of the deployed algorithms. Recently, the security vulnerability of DL algorithms to adversarial samples has been widely recognized. The fabricated samples can lead to various misbehaviors of the DL models while being perceived as benign by humans. Successful implementations of adversarial attacks in real physical-world scenarios further demonstrate their practicality. Hence, adversarial attack and defense techniques have attracted increasing attention from both machine learning and security communities and have become a hot research topic in recent years. In this paper, we first introduce the theoretical foundations, algorithms, and applications of adversarial attack techniques. We then describe a few research efforts on the defense techniques, which cover the broad frontier in the field. Several open problems and challenges are subsequently discussed, which we hope will provoke further research efforts in this critical area."
}

@article{kwon_selective_2019,
	title = {Selective {Untargeted} {Evasion} {Attack}: {An} adversarial example that will not be classified as certain avoided classes},
	issn = {2169-3536},
	shorttitle = {Selective {Untargeted} {Evasion} {Attack}},
	url = {https://ieeexplore.ieee.org/document/8727886/},
	doi = {10.1109/ACCESS.2019.2920410},
	urldate = {2020-01-16},
	journal = {IEEE Access},
	author = {Kwon, Hyun and Kim, Yongchul and Yoon, Hyunsoo and Choi, Daeseon},
	year = {2019},
	pages = {1--1}
}

@article{wang_robust_2019,
	title = {Robust {Pervasive} {Detection} for {Adversarial} {Samples} of {Artificial} {Intelligence} in {IoT} {Environments}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8725605/},
	doi = {10.1109/ACCESS.2019.2919695},
	urldate = {2020-01-16},
	journal = {IEEE Access},
	author = {Wang, Shen and Qiao, Zhuobiao},
	year = {2019},
	pages = {88693--88704},
	file = {Full Text:/Users/maumau/Zotero/storage/NGNJMUJR/Wang and Qiao - 2019 - Robust Pervasive Detection for Adversarial Samples.pdf:application/pdf}
}

@article{xiang_study_2019,
	title = {Study of {Sensitivity} to {Weight} {Perturbation} for {Convolution} {Neural} {Network}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8755296/},
	doi = {10.1109/ACCESS.2019.2926768},
	urldate = {2020-01-16},
	journal = {IEEE Access},
	author = {Xiang, Lin and Zeng, Xiaoqin and Niu, Yuhu and Liu, Yanjun},
	year = {2019},
	pages = {93898--93908},
	file = {Full Text:/Users/maumau/Zotero/storage/52U2RQ96/Xiang et al. - 2019 - Study of Sensitivity to Weight Perturbation for Co.pdf:application/pdf}
}

@article{mahdizadehaghdam_deep_2019,
	title = {Deep {Dictionary} {Learning}: {A} {PARametric} {NETwork} {Approach}},
	volume = {28},
	issn = {1057-7149, 1941-0042},
	shorttitle = {Deep {Dictionary} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8708973/},
	doi = {10.1109/TIP.2019.2914376},
	number = {10},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Image Processing},
	author = {Mahdizadehaghdam, Shahin and Panahi, Ashkan and Krim, Hamid and Dai, Liyi},
	month = oct,
	year = {2019},
	pages = {4790--4802},
	file = {Submitted Version:/Users/maumau/Zotero/storage/UJWE5GRE/Mahdizadehaghdam et al. - 2019 - Deep Dictionary Learning A PARametric NETwork App.pdf:application/pdf}
}

@inproceedings{liu_query-free_2019,
	address = {Shanghai, China},
	title = {Query-{Free} {Embedding} {Attack} {Against} {Deep} {Learning}},
	isbn = {978-1-5386-9552-4},
	url = {https://ieeexplore.ieee.org/document/8784813/},
	doi = {10.1109/ICME.2019.00073},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
	publisher = {IEEE},
	author = {Liu, Yujia and Zhang, Weiming and Yu, Nenghai},
	month = jul,
	year = {2019},
	pages = {380--386}
}

@inproceedings{asadi_novel_2019,
	address = {Yazd, Iran},
	title = {A {Novel} {Image} {Perturbation} {Approach}: {Perturbing} {Latent} {Representation}},
	isbn = {978-1-72811-508-5},
	shorttitle = {A {Novel} {Image} {Perturbation} {Approach}},
	url = {https://ieeexplore.ieee.org/document/8786388/},
	doi = {10.1109/IranianCEE.2019.8786388},
	urldate = {2020-01-16},
	booktitle = {2019 27th {Iranian} {Conference} on {Electrical} {Engineering} ({ICEE})},
	publisher = {IEEE},
	author = {Asadi, Nader and Eftekhari, Mahdi},
	month = apr,
	year = {2019},
	pages = {1895--1899}
}

@inproceedings{liu_analyzing_2018,
	address = {Berlin, Germany},
	title = {Analyzing the {Noise} {Robustness} of {Deep} {Neural} {Networks}},
	isbn = {978-1-5386-6861-0},
	url = {https://ieeexplore.ieee.org/document/8802509/},
	doi = {10.1109/VAST.2018.8802509},
	urldate = {2020-01-16},
	booktitle = {2018 {IEEE} {Conference} on {Visual} {Analytics} {Science} and {Technology} ({VAST})},
	publisher = {IEEE},
	author = {Liu, Mengchen and Liu, Shixia and Su, Hang and Cao, Kelei and Zhu, Jun},
	month = oct,
	year = {2018},
	pages = {60--71}
}

@inproceedings{suzuki_adversarial_2019,
	address = {Wellington, New Zealand},
	title = {Adversarial {Example} {Generation} using {Evolutionary} {Multi}-objective {Optimization}},
	isbn = {978-1-72812-153-6},
	url = {https://ieeexplore.ieee.org/document/8790123/},
	doi = {10.1109/CEC.2019.8790123},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Suzuki, Takahiro and Takeshita, Shingo and Ono, Satoshi},
	month = jun,
	year = {2019},
	pages = {2136--2144}
}

@inproceedings{li_universal_2019-2,
	address = {Wellington, New Zealand},
	title = {Universal {Rules} for {Fooling} {Deep} {Neural} {Networks} based {Text} {Classification}},
	isbn = {978-1-72812-153-6},
	url = {https://ieeexplore.ieee.org/document/8790213/},
	doi = {10.1109/CEC.2019.8790213},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {Congress} on {Evolutionary} {Computation} ({CEC})},
	publisher = {IEEE},
	author = {Li, Di and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	month = jun,
	year = {2019},
	pages = {2221--2228}
}

@inproceedings{hossain_distortion_2019,
	address = {Taipei, Taiwan},
	title = {Distortion {Robust} {Image} {Classification} {Using} {Deep} {Convolutional} {Neural} {Network} with {Discrete} {Cosine} {Transform}},
	isbn = {978-1-5386-6249-6},
	url = {https://ieeexplore.ieee.org/document/8803787/},
	doi = {10.1109/ICIP.2019.8803787},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	publisher = {IEEE},
	author = {Hossain, Md Tahmid and Teng, Shyh Wei and Zhang, Dengsheng and Lim, Suryani and Lu, Guojun},
	month = sep,
	year = {2019},
	pages = {659--663},
	file = {Submitted Version:/Users/maumau/Zotero/storage/BL5TSAH4/Hossain et al. - 2019 - Distortion Robust Image Classification Using Deep .pdf:application/pdf}
}

@inproceedings{you_adversarial_2019,
	address = {Taipei, Taiwan},
	title = {Adversarial {Noise} {Layer}: {Regularize} {Neural} {Network} by {Adding} {Noise}},
	isbn = {978-1-5386-6249-6},
	shorttitle = {Adversarial {Noise} {Layer}},
	url = {https://ieeexplore.ieee.org/document/8803055/},
	doi = {10.1109/ICIP.2019.8803055},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	publisher = {IEEE},
	author = {You, Zhonghui and Ye, Jinmian and Li, Kunming and Xu, Zenglin and Wang, Ping},
	month = sep,
	year = {2019},
	pages = {909--913},
	file = {Submitted Version:/Users/maumau/Zotero/storage/SMKA4VNE/You et al. - 2019 - Adversarial Noise Layer Regularize Neural Network.pdf:application/pdf}
}

@article{yuan_adversarial_2019-1,
	title = {Adversarial {Examples}: {Attacks} and {Defenses} for {Deep} {Learning}},
	volume = {30},
	issn = {2162-237X, 2162-2388},
	shorttitle = {Adversarial {Examples}},
	url = {https://ieeexplore.ieee.org/document/8611298/},
	doi = {10.1109/TNNLS.2018.2886017},
	number = {9},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Yuan, Xiaoyong and He, Pan and Zhu, Qile and Li, Xiaolin},
	month = sep,
	year = {2019},
	pages = {2805--2824},
	file = {Submitted Version:/Users/maumau/Zotero/storage/K47GEF3U/Yuan et al. - 2019 - Adversarial Examples Attacks and Defenses for Dee.pdf:application/pdf}
}

@article{mopuri_generalizable_2019,
	title = {Generalizable {Data}-{Free} {Objective} for {Crafting} {Universal} {Adversarial} {Perturbations}},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/8423654/},
	doi = {10.1109/TPAMI.2018.2861800},
	number = {10},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Mopuri, Konda Reddy and Ganeshan, Aditya and Babu, R. Venkatesh},
	month = oct,
	year = {2019},
	pages = {2452--2465},
	file = {Submitted Version:/Users/maumau/Zotero/storage/WVXQDF6C/Mopuri et al. - 2019 - Generalizable Data-Free Objective for Crafting Uni.pdf:application/pdf}
}

@inproceedings{ling_deepsec_2019,
	address = {San Francisco, CA, USA},
	title = {{DEEPSEC}: {A} {Uniform} {Platform} for {Security} {Analysis} of {Deep} {Learning} {Model}},
	isbn = {978-1-5386-6660-9},
	shorttitle = {{DEEPSEC}},
	url = {https://ieeexplore.ieee.org/document/8835375/},
	doi = {10.1109/SP.2019.00023},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Ling, Xiang and Ji, Shouling and Zou, Jiaxu and Wang, Jiannan and Wu, Chunming and Li, Bo and Wang, Ting},
	month = may,
	year = {2019},
	pages = {673--690}
}

@inproceedings{wang_neural_2019,
	address = {San Francisco, CA, USA},
	title = {Neural {Cleanse}: {Identifying} and {Mitigating} {Backdoor} {Attacks} in {Neural} {Networks}},
	isbn = {978-1-5386-6660-9},
	shorttitle = {Neural {Cleanse}},
	url = {https://ieeexplore.ieee.org/document/8835365/},
	doi = {10.1109/SP.2019.00031},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	publisher = {IEEE},
	author = {Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y.},
	month = may,
	year = {2019},
	pages = {707--723}
}

@article{kwon_selective_2020,
	title = {Selective {Audio} {Adversarial} {Example} in {Evasion} {Attack} on {Speech} {Recognition} {System}},
	volume = {15},
	issn = {1556-6013, 1556-6021},
	url = {https://ieeexplore.ieee.org/document/8747397/},
	doi = {10.1109/TIFS.2019.2925452},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Kwon, Hyun and Kim, Yongchul and Yoon, Hyunsoo and Choi, Daeseon},
	year = {2020},
	pages = {526--538}
}

@inproceedings{katzir_detecting_2019,
	address = {Budapest, Hungary},
	title = {Detecting {Adversarial} {Perturbations} {Through} {Spatial} {Behavior} in {Activation} {Spaces}},
	isbn = {978-1-72811-985-4},
	url = {https://ieeexplore.ieee.org/document/8852285/},
	doi = {10.1109/IJCNN.2019.8852285},
	urldate = {2020-01-16},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Katzir, Ziv and Elovici, Yuval},
	month = jul,
	year = {2019},
	pages = {1--9}
}

@inproceedings{ishii_training_2019,
	address = {Budapest, Hungary},
	title = {Training {Deep} {Neural} {Networks} with {Adversarially} {Augmented} {Features} for {Small}-scale {Training} {Datasets}},
	isbn = {978-1-72811-985-4},
	url = {https://ieeexplore.ieee.org/document/8852250/},
	doi = {10.1109/IJCNN.2019.8852250},
	urldate = {2020-01-16},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Ishii, Masato and Sato, Atsushi},
	month = jul,
	year = {2019},
	pages = {1--8}
}

@inproceedings{yuan_adversarial_2019-2,
	address = {Budapest, Hungary},
	title = {Adversarial {Collaborative} {Auto}-encoder for {Top}-{N} {Recommendation}},
	isbn = {978-1-72811-985-4},
	url = {https://ieeexplore.ieee.org/document/8851902/},
	doi = {10.1109/IJCNN.2019.8851902},
	urldate = {2020-01-16},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Yuan, Feng and Yao, Lina and Benatallah, Boualem},
	month = jul,
	year = {2019},
	pages = {1--8}
}

@inproceedings{zheng_targeted_2019,
	address = {Budapest, Hungary},
	title = {Targeted {Black}-{Box} {Adversarial} {Attack} {Method} for {Image} {Classification} {Models}},
	isbn = {978-1-72811-985-4},
	url = {https://ieeexplore.ieee.org/document/8852078/},
	doi = {10.1109/IJCNN.2019.8852078},
	urldate = {2020-01-16},
	booktitle = {2019 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	publisher = {IEEE},
	author = {Zheng, Su and Chen, Jialin and Wang, Lingli},
	month = jul,
	year = {2019},
	pages = {1--8}
}

@article{dai_backdoor_2019,
	title = {A {Backdoor} {Attack} {Against} {LSTM}-{Based} {Text} {Classification} {Systems}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8836465/},
	doi = {10.1109/ACCESS.2019.2941376},
	urldate = {2020-01-16},
	journal = {IEEE Access},
	author = {Dai, Jiazhu and Chen, Chuanshuai and Li, Yufeng},
	year = {2019},
	pages = {138872--138878},
	file = {Full Text:/Users/maumau/Zotero/storage/D2SMC7KZ/Dai et al. - 2019 - A Backdoor Attack Against LSTM-Based Text Classifi.pdf:application/pdf}
}

@article{su_one_2019,
	title = {One {Pixel} {Attack} for {Fooling} {Deep} {Neural} {Networks}},
	volume = {23},
	issn = {1089-778X, 1089-778X, 1941-0026},
	url = {https://ieeexplore.ieee.org/document/8601309/},
	doi = {10.1109/TEVC.2019.2890858},
	number = {5},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
	month = oct,
	year = {2019},
	pages = {828--841},
	file = {Submitted Version:/Users/maumau/Zotero/storage/GGF4UQK2/Su et al. - 2019 - One Pixel Attack for Fooling Deep Neural Networks.pdf:application/pdf}
}

@inproceedings{zhu_defense_2019,
	address = {Guangzhou, China},
	title = {Defense against {Adversarial} {Vision} {Perturbations} via {Subspace} {Diagnosis}},
	isbn = {978-988-15639-7-2},
	url = {https://ieeexplore.ieee.org/document/8865376/},
	doi = {10.23919/ChiCC.2019.8865376},
	urldate = {2020-01-16},
	booktitle = {2019 {Chinese} {Control} {Conference} ({CCC})},
	publisher = {IEEE},
	author = {Zhu, Jinlin and Peng, Guohao and Fu, Wenhao and Wang, Danwei},
	month = jul,
	year = {2019},
	pages = {8665--8670}
}

@inproceedings{van_de_kamp_economy_2019,
	address = {Istanbul, Turkey},
	title = {{ECONoMy}: {Ensemble} {Collaborative} {Learning} {Using} {Masking}},
	isbn = {978-1-5386-9358-2},
	shorttitle = {{ECONoMy}},
	url = {https://ieeexplore.ieee.org/document/8880822/},
	doi = {10.1109/PIMRCW.2019.8880822},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} 30th {International} {Symposium} on {Personal}, {Indoor} and {Mobile} {Radio} {Communications} ({PIMRC} {Workshops})},
	publisher = {IEEE},
	author = {Van De Kamp, Lars and Ugwuoke, Chibuike and Erkin, Zekeriya},
	month = sep,
	year = {2019},
	pages = {1--6}
}

@inproceedings{rofail_multi-modal_2019,
	address = {Cairo, Egypt},
	title = {Multi-{Modal} {Deep} {Learning} for {Vehicle} {Sensor} {Data} {Abstraction} and {Attack} {Detection}},
	isbn = {978-1-72813-473-4},
	url = {https://ieeexplore.ieee.org/document/8906405/},
	doi = {10.1109/ICVES.2019.8906405},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE} {International} {Conference} on {Vehicular} {Electronics} and {Safety} ({ICVES})},
	publisher = {IEEE},
	author = {Rofail, Mark and Alsafty, Aysha and Matousek, Matthias and Kargl, Frank},
	month = sep,
	year = {2019},
	pages = {1--7}
}

@article{mustafa_image_2020-1,
	title = {Image {Super}-{Resolution} as a {Defense} {Against} {Adversarial} {Attacks}},
	volume = {29},
	issn = {1057-7149, 1941-0042},
	url = {https://ieeexplore.ieee.org/document/8844865/},
	doi = {10.1109/TIP.2019.2940533},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Image Processing},
	author = {Mustafa, Aamir and Khan, Salman H. and Hayat, Munawar and Shen, Jianbing and Shao, Ling},
	year = {2020},
	pages = {1711--1724},
	file = {Submitted Version:/Users/maumau/Zotero/storage/3AK7HHFK/Mustafa et al. - 2020 - Image Super-Resolution as a Defense Against Advers.pdf:application/pdf}
}

@article{balda_perturbation_2019,
	title = {Perturbation {Analysis} of {Learning} {Algorithms}: {Generation} of {Adversarial} {Examples} {From} {Classification} to {Regression}},
	volume = {67},
	issn = {1053-587X, 1941-0476},
	shorttitle = {Perturbation {Analysis} of {Learning} {Algorithms}},
	url = {https://ieeexplore.ieee.org/document/8846746/},
	doi = {10.1109/TSP.2019.2943232},
	number = {23},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Signal Processing},
	author = {Balda, Emilio Rafael and Behboodi, Arash and Mathar, Rudolf},
	month = dec,
	year = {2019},
	pages = {6078--6091}
}

@article{flowers_evaluating_2020,
	title = {Evaluating {Adversarial} {Evasion} {Attacks} in the {Context} of {Wireless} {Communications}},
	volume = {15},
	issn = {1556-6013, 1556-6021},
	url = {https://ieeexplore.ieee.org/document/8792120/},
	doi = {10.1109/TIFS.2019.2934069},
	urldate = {2020-01-16},
	journal = {IEEE Transactions on Information Forensics and Security},
	author = {Flowers, Bryse and Buehrer, R. Michael and Headley, William C.},
	year = {2020},
	pages = {1102--1113},
	file = {Submitted Version:/Users/maumau/Zotero/storage/3GAF3EVB/Flowers et al. - 2020 - Evaluating Adversarial Evasion Attacks in the Cont.pdf:application/pdf}
}

@article{yan_robust_2019,
	title = {A {Robust} {Deep}-{Neural}-{Network}-{Based} {Compressed} {Model} for {Mobile} {Device} {Assisted} by {Edge} {Server}},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8928622/},
	doi = {10.1109/ACCESS.2019.2958406},
	urldate = {2020-01-16},
	journal = {IEEE Access},
	author = {Yan, Yushuang and Pei, Qingqi},
	year = {2019},
	pages = {179104--179117},
	file = {Full Text:/Users/maumau/Zotero/storage/6GNJ2AT4/Yan and Pei - 2019 - A Robust Deep-Neural-Network-Based Compressed Mode.pdf:application/pdf}
}

@article{qadir_improving_2020,
	title = {Improving {Automatic} {Polyp} {Detection} {Using} {CNN} by {Exploiting} {Temporal} {Dependency} in {Colonoscopy} {Video}},
	volume = {24},
	issn = {2168-2194, 2168-2208},
	url = {https://ieeexplore.ieee.org/document/8678826/},
	doi = {10.1109/JBHI.2019.2907434},
	number = {1},
	urldate = {2020-01-16},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Qadir, Hemin Ali and Balasingham, Ilangko and Solhusvik, Johannes and Bergsland, Jacob and Aabakken, Lars and Shin, Younghak},
	month = jan,
	year = {2020},
	pages = {180--193}
}

@inproceedings{inkawhich_feature_2019,
	address = {Long Beach, CA, USA},
	title = {Feature {Space} {Perturbations} {Yield} {More} {Transferable} {Adversarial} {Examples}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953700/},
	doi = {10.1109/CVPR.2019.00723},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Inkawhich, Nathan and Wen, Wei and Li, Hai Helen and Chen, Yiran},
	month = jun,
	year = {2019},
	pages = {7059--7067}
}

@inproceedings{tsuzuku_structural_2019-1,
	address = {Long Beach, CA, USA},
	title = {On the {Structural} {Sensitivity} of {Deep} {Convolutional} {Networks} to the {Directions} of {Fourier} {Basis} {Functions}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954086/},
	doi = {10.1109/CVPR.2019.00014},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tsuzuku, Yusuke and Sato, Issei},
	month = jun,
	year = {2019},
	pages = {51--60}
}

@inproceedings{yao_trust_2019,
	address = {Long Beach, CA, USA},
	title = {Trust {Region} {Based} {Adversarial} {Attack} on {Neural} {Networks}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953547/},
	doi = {10.1109/CVPR.2019.01161},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yao, Zhewei and Gholami, Amir and Xu, Peng and Keutzer, Kurt and Mahoney, Michael W.},
	month = jun,
	year = {2019},
	pages = {11342--11351}
}

@inproceedings{modas_sparsefool_2019,
	address = {Long Beach, CA, USA},
	title = {{SparseFool}: {A} {Few} {Pixels} {Make} a {Big} {Difference}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{SparseFool}},
	url = {https://ieeexplore.ieee.org/document/8954332/},
	doi = {10.1109/CVPR.2019.00930},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Modas, Apostolos and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	month = jun,
	year = {2019},
	pages = {9079--9088}
}

@inproceedings{dong_evading_2019,
	address = {Long Beach, CA, USA},
	title = {Evading {Defenses} to {Transferable} {Adversarial} {Examples} by {Translation}-{Invariant} {Attacks}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953425/},
	doi = {10.1109/CVPR.2019.00444},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Dong, Yinpeng and Pang, Tianyu and Su, Hang and Zhu, Jun},
	month = jun,
	year = {2019},
	pages = {4307--4316}
}

@inproceedings{zeng_adversarial_2019,
	address = {Long Beach, CA, USA},
	title = {Adversarial {Attacks} {Beyond} the {Image} {Space}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954178/},
	doi = {10.1109/CVPR.2019.00443},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zeng, Xiaohui and Liu, Chenxi and Wang, Yu-Siang and Qiu, Weichao and Xie, Lingxi and Tai, Yu-Wing and Tang, Chi-Keung and Yuille, Alan L.},
	month = jun,
	year = {2019},
	pages = {4297--4306}
}

@inproceedings{collomosse_livesketch_2019,
	address = {Long Beach, CA, USA},
	title = {{LiveSketch}: {Query} {Perturbations} for {Guided} {Sketch}-{Based} {Visual} {Search}},
	isbn = {978-1-72813-293-8},
	shorttitle = {{LiveSketch}},
	url = {https://ieeexplore.ieee.org/document/8953583/},
	doi = {10.1109/CVPR.2019.00299},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Collomosse, John and Bui, Tu and Jin, Hailin},
	month = jun,
	year = {2019},
	pages = {2874--2882}
}

@inproceedings{dong_efficient_2019,
	address = {Long Beach, CA, USA},
	title = {Efficient {Decision}-{Based} {Black}-{Box} {Adversarial} {Attacks} on {Face} {Recognition}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953400/},
	doi = {10.1109/CVPR.2019.00790},
	urldate = {2020-01-16},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Dong, Yinpeng and Su, Hang and Wu, Baoyuan and Li, Zhifeng and Liu, Wei and Zhang, Tong and Zhu, Jun},
	month = jun,
	year = {2019},
	pages = {7706--7714}
}

@inproceedings{he_deep_2019,
	address = {Beijing, China},
	title = {Deep {Minimax} {Probability} {Machine}},
	isbn = {978-1-72814-896-0},
	url = {https://ieeexplore.ieee.org/document/8955575/},
	doi = {10.1109/ICDMW.2019.00127},
	urldate = {2020-01-16},
	booktitle = {2019 {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	publisher = {IEEE},
	author = {He, Lirong and Guo, Ziyi and Huang, Kaizhu and Xu, Zenglin},
	month = nov,
	year = {2019},
	pages = {869--876}
}

@book{martin_abadi_tensorflow_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	year = {2015},
	annote = {Software available from tensorflow.org}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	language = {en},
	number = {6},
	urldate = {2020-01-19},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	pages = {84--90},
	file = {Full Text:/Users/maumau/Zotero/storage/VDIHDM5B/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@techreport{rosenblatt_perceptron_1957,
	address = {Buffalo, NY},
	title = {The perceptron, a perceiving and recognizing automaton : ({Project} {Para})},
	shorttitle = {The perceptron, a perceiving and recognizing automaton},
	institution = {Cornell Aeronautical Laboratory},
	author = {Rosenblatt, F.},
	year = {1957},
	file = {The perceptron a perceiving and recognizing automaton Project - Technische Informationsbibliothek (TIB):/Users/maumau/Zotero/storage/LKQCIRQY/The-perceptron-a-perceiving-and-recognizing-automaton.html:text/html}
}

@book{aurelien_geron_hands-machine_2017,
	edition = {1.},
	title = {Hands-{On} {Machine} {Learning} with {Scikit}-{Learn} \& {Tensorflow}},
	isbn = {978-1-4919-6229-9},
	publisher = {O'Reilly Media Inc.},
	author = {{Aurélien Géron}},
	month = {März},
	year = {2017}
}

@article{papernot_transferability_2016,
	title = {Transferability in {Machine} {Learning}: from {Phenomena} to {Black}-{Box} {Attacks} using {Adversarial} {Samples}},
	shorttitle = {Transferability in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1605.07277},
	abstract = {Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19\% misclassification rate) and Google (88.94\%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.},
	urldate = {2020-02-02},
	journal = {arXiv:1605.07277 [cs]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian},
	month = {Mai},
	year = {2016},
	note = {arXiv: 1605.07277},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/8L59HJ4K/Papernot et al. - 2016 - Transferability in Machine Learning from Phenomen.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/DAVX75RK/1605.html:text/html}
}

@article{shao_transfer_2015,
	title = {Transfer {Learning} for {Visual} {Categorization}: {A} {Survey}},
	volume = {26},
	issn = {2162-2388},
	shorttitle = {Transfer {Learning} for {Visual} {Categorization}},
	doi = {10.1109/TNNLS.2014.2330900},
	abstract = {Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.},
	number = {5},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Shao, Ling and Zhu, Fan and Li, Xuelong},
	month = {Mai},
	year = {2015},
	keywords = {learning (artificial intelligence), object recognition, Training, machine learning, Testing, Humans, image classification, Algorithms, Learning systems, Neural Networks (Computer), survey, Training data, Visualization, Adaptation models, Action recognition, human action recognition, Knowledge, Knowledge transfer, Machine Learning, Models, Theoretical, Surveys and Questionnaires, Transfer (Psychology), transfer learning, transfer learning algorithms, visual categorization, visual categorization., Visual Perception},
	pages = {1019--1034},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/HB6RT8UR/6847217.html:text/html}
}

@article{wiyatno_adversarial_2019,
	title = {Adversarial {Examples} in {Modern} {Machine} {Learning}: {A} {Review}},
	shorttitle = {Adversarial {Examples} in {Modern} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1911.05268},
	abstract = {Recent research has found that many families of machine learning models are vulnerable to adversarial examples: inputs that are specifically designed to cause the target model to produce erroneous outputs. In this survey, we focus on machine learning models in the visual domain, where methods for generating and detecting such examples have been most extensively studied. We explore a variety of adversarial attack methods that apply to image-space content, real world adversarial attacks, adversarial defenses, and the transferability property of adversarial examples. We also discuss strengths and weaknesses of various methods of adversarial attack and defense. Our aim is to provide an extensive coverage of the field, furnishing the reader with an intuitive understanding of the mechanics of adversarial attack and defense mechanisms and enlarging the community of researchers studying this fundamental set of problems.},
	urldate = {2020-02-19},
	journal = {arXiv:1911.05268 [cs, stat]},
	author = {Wiyatno, Rey Reza and Xu, Anqi and Dia, Ousmane and de Berker, Archy},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.05268},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	annote = {Comment: Work in progress, 97 pages},
	file = {arXiv Fulltext PDF:/Users/maumau/Zotero/storage/39ADD7UH/Wiyatno et al. - 2019 - Adversarial Examples in Modern Machine Learning A.pdf:application/pdf;arXiv.org Snapshot:/Users/maumau/Zotero/storage/KKB7W4B5/1911.html:text/html}
}

@book{rolf_socher_mathematik_2011,
	address = {München},
	edition = {1.},
	title = {Mathematik für {Informatiker}},
	isbn = {978-3-446-42254-4},
	publisher = {Carl Hanser Verlag GmbH \& Co. KG},
	author = {{Rolf Socher}},
	year = {2011}
}

@article{jiao_survey_2019,
	title = {A {Survey} on the {New} {Generation} of {Deep} {Learning} in {Image} {Processing}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2956508},
	abstract = {During the past decade, deep learning is one of the essential breakthroughs made in artificial intelligence. In particular, it has achieved great success in image processing. Correspondingly, various applications related to image processing are also promoting the rapid development of deep learning in all aspects of network structure, layer designing, and training tricks. However, the deeper structure makes the back-propagation algorithm more difficult. At the same time, the scale of training images without labels is also rapidly increasing, and class imbalance severely affects the performance of deep learning, these urgently require more novelty deep models and new parallel computing system to more effectively interpret the content of the image and form a suitable analysis mechanism. In this context, this survey provides four deep learning model series, which includes CNN series, GAN series, ELM-RVFL series, and other series, for comprehensive understanding towards the analytical techniques of image processing field, clarify the most important advancements and shed some light on future studies. By further studying the relationship between deep learning and image processing tasks, which can not only help us understand the reasons for the success of deep learning but also inspires new deep models and training methods. More importantly, this survey aims to improve or arouse other researchers to catch a glimpse of the state-of-the-art deep learning methods in the field of image processing and facilitate the applications of these deep learning technologies in their research tasks. Besides, we discuss the open issues and the promising directions of future research in image processing using the new generation of deep learning.},
	journal = {IEEE Access},
	author = {Jiao, Licheng and Zhao, Jin},
	year = {2019},
	keywords = {learning (artificial intelligence), Convolutional neural networks, deep learning, Mathematical model, extreme learning machine, Machine learning, generative adversarial network, Generative adversarial networks, image classification, object detection, Task analysis, image processing, convolutional neural network, neural net architecture, ADMM-Net, back-propagation algorithm, capsule networks, CNN series, deep forest, deep learning model series, deep learning technologies, ELM-RVFL series, GAN series, Image processing, image processing field, Image resolution, novelty deep models, state-of-the-art deep learning methods, style transfer, super-resolution, training methods},
	pages = {172231--172263},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/64NMPH87/8917633.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/BX526C2F/Jiao and Zhao - 2019 - A Survey on the New Generation of Deep Learning in.pdf:application/pdf}
}

@book{francois_chollet_deep_nodate,
	address = {Frechen},
	edition = {1.},
	title = {Deep {Learning} mit {Python} und {Keras}},
	isbn = {978-3-95845-838-3},
	publisher = {mitp Verlags GmbH \& Co. KG},
	author = {{Francois Chollet}}
}

@book{kaballo_grundkurs_2018,
	address = {Berlin, Heidelberg},
	edition = {2.},
	title = {Grundkurs {Funktionalanalysis}},
	isbn = {978-3-662-54748-9},
	url = {https://doi.org/10.1007/978-3-662-54748-9_1},
	abstract = {In diesem Kapitel werden Banachräume, d. h. vollständige normierte Räume, vorgestellt; stetige lineare Operatoren zwischen Banachräumen folgen ab Kap. 3. Die Untersuchung dieser für die Funktionalanalysis grundlegenden Konzepte erfolgt durch ein Zusammenspiel algebraischer und analytischer Methoden.},
	publisher = {Springer Spektrum},
	author = {Kaballo, Winfried},
	year = {2018},
	doi = {10.1007/978-3-662-54748-9_1}
}

@inproceedings{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	booktitle = {{ICML}},
	author = {Tan, Mingxing and Le, Quoc V.},
	year = {2019}
}

@inproceedings{huang_densely_2017,
	title = {Densely {Connected} {Convolutional} {Networks}},
	doi = {10.1109/CVPR.2017.243},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian},
	year = {2017}
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = {Juni},
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {learning (artificial intelligence), neural nets, Training, Neural networks, image classification, object detection, Visualization, image recognition, Image segmentation, CIFAR-10, COCO object detection dataset, COCO segmentation, Complexity theory, deep residual learning, deep residual nets, deeper neural network training, Degradation, ILSVRC \& COCO 2015 competitions, ILSVRC 2015 classification task, Image recognition, ImageNet dataset, ImageNet localization, ImageNet test set, residual function learning, residual nets, VGG nets, visual recognition tasks},
	pages = {770--778},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/7KHBAPU4/7780459.html:text/html;Submitted Version:/Users/maumau/Zotero/storage/U3S82YVG/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = {Juni},
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {convolution, Neural networks, feature extraction, Sparse matrices, Computer vision, image classification, object detection, Computer architecture, Visualization, neural net architecture, architectural decision, Convolutional codes, convolutional neural network architecture, decision making, Hebbian learning, Hebbian principle, object classification, Object detection, resource allocation, resource utilization},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/LFMPYAHV/7298594.html:text/html;Submitted Version:/Users/maumau/Zotero/storage/3FH2FPM3/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:application/pdf}
}

@inproceedings{moosavi-dezfooli_deepfool_2016,
	title = {{DeepFool}: {A} {Simple} and {Accurate} {Method} to {Fool} {Deep} {Neural} {Networks}},
	shorttitle = {{DeepFool}},
	doi = {10.1109/CVPR.2016.282},
	abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	month = {Juni},
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {neural nets, deep neural networks, Neural networks, Robustness, Computer vision, Algorithm design and analysis, image classification, Optimization, adversarial perturbations, deep classifiers, DeepFool algorithm, image classification tasks, Level set, Pattern recognition, perturbation techniques, robust classifiers},
	pages = {2574--2582},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/ISP8Q5LA/7780651.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/WA47EN5K/Moosavi-Dezfooli et al. - 2016 - DeepFool A Simple and Accurate Method to Fool Dee.pdf:application/pdf}
}

@inproceedings{moosavi-dezfooli_universal_2017-1,
	title = {Universal {Adversarial} {Perturbations}},
	doi = {10.1109/CVPR.2017.17},
	abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	month = {Juli},
	year = {2017},
	note = {ISSN: 1063-6919},
	keywords = {neural nets, deep neural networks, Training, Neural networks, probability, Robustness, image classification, Optimization, Computer architecture, Visualization, universal adversarial perturbations, Correlation, deep neural network classifier, image-agnostic, natural images, universal perturbations, very small perturbation vector},
	pages = {86--94},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/XA9ASPFK/8099500.html:text/html;IEEE Xplore Full Text PDF:/Users/maumau/Zotero/storage/FH2H3LFU/Moosavi-Dezfooli et al. - 2017 - Universal Adversarial Perturbations.pdf:application/pdf}
}


@inproceedings{imagenet_cvpr09,
  author    = {Jia Deng and
               Wei Dong and
               Richard Socher and
               Li{-}Jia Li and
               Kai Li and
               Fei{-}Fei Li},
  title     = {ImageNet: {A} large-scale hierarchical image database},
  pages     = {248--255},
  publisher = {{IEEE} Computer Society},
  year      = {2009},
  url       = {https://doi.org/10.1109/CVPR.2009.5206848},
  doi       = {10.1109/CVPR.2009.5206848},
  timestamp = {Fri, 27 Mar 2020 08:56:42 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/DengDSLL009.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{isakov_survey_2019,
	title = {Survey of {Attacks} and {Defenses} on {Edge}-{Deployed} {Neural} {Networks}},
	doi = {10.1109/HPEC.2019.8916519},
	abstract = {Deep Neural Network (DNN) workloads are quickly moving from datacenters onto edge devices, for latency, privacy, or energy reasons. While datacenter networks can be protected using conventional cybersecurity measures, edge neural networks bring a host of new security challenges. Unlike classic IoT applications, edge neural networks are typically very compute and memory intensive, their execution is data-independent, and they are robust to noise and faults. Neural network models may be very expensive to develop, and can potentially reveal information about the private data they were trained on, requiring special care in distribution. The hidden states and outputs of the network can also be used in reconstructing user inputs, potentially violating users' privacy. Furthermore, neural networks are vulnerable to adversarial attacks, which may cause misclassifications and violate the integrity of the output. These properties add challenges when securing edge-deployed DNNs, requiring new considerations, threat models, priorities, and approaches in securely and privately deploying DNNs to the edge. In this work, we cover the landscape of attacks on, and defenses, of neural networks deployed in edge devices and provide a taxonomy of attacks and defenses targeting edge DNNs.},
	booktitle = {2019 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Isakov, Mihailo and Gadepally, Vijay and Gettings, Karen M. and Kinsy, Michel A.},
	month = {September},
	year = {2019},
	note = {ISSN: 2377-6943},
	keywords = {neural nets, Training, Mathematical model, Neural networks, Data models, security of data, neural networks, Internet of Things, adversarial attacks, cybersecurity, Taxonomy, data privacy, Data privacy, computer centres, datacenter networks, deep neural network workloads, defenses, edge-deployed neural networks, IoT applications, private data, securing edge-deployed DNNs, security, Software},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/Users/maumau/Zotero/storage/UTRBQN8C/8916519.html:text/html;Submitted Version:/Users/maumau/Zotero/storage/QIZXVG23/Isakov et al. - 2019 - Survey of Attacks and Defenses on Edge-Deployed Ne.pdf:application/pdf}
}